
## Expected Input Structure


Lets say I have a 10 second video, which is sampled at 2 fps.

So I will have a total of 20 frames.

### Model Input

At every frame, the model will receive the following inputs:

- The current frame
- A list of previous frames that fit the context window
- A list of previous conversation turns that fit the context window
- The current dialog state

Proassist handled context window management by doing iterative progress summarization.
However, we are using a DST to represent the dialog state and the model is always informed about what it has done so far and what it is left to do.

In our approach, when the model reaches the context limit, we will have to drop a certain percentage of tokens(both image tokens and text tokens, depending on the temporal order) from the beginning of the sequence. 
We can use a sliding window approach to drop the tokens. If there is a frame with incomplete text, we can drop the entire frame.
This will allow the model to continue processing the video without losing too much context.
In proassist, it completely clears the cache and starts from scratch using the current frame and the progress summary.
It also has to generate progress summary, whereas in our case we have no additional processing, we just check the token count and drop a certain amount of tokens from the context. 

### Binary Decision Heads

Now the initial task is to have the binary decision at every frame, so I expect a list of 20 items with the binary decision.

The model has 2 binary heads, speaking decision and dst update decision.

I want to visualize the output of each head.

Does my current implementation follow this structure for the binary decisions.


### Generations

At any frame, if the model decides to speak, then it should generate the text.

Similarly, if the model decides to update the dst, then it should generate the dst update.

The DST will be built on the recipe, so the updates will only take place during step transitions, not at every frame.
The DST update will also be a delta of what changed, not the whole dst prediction.

I want to visualize the output of the generation head.

So basically the output will be a list of 20 items, where each item is a dictionary with the following keys:

- speaking_decision: 0 or 1
- dst_update_decision: 0 or 1
- text: if speaking_decision is 1, then the text generated by the model
- dst_update: if dst_update_decision is 1, then the dst update generated by the model





