# ProAssist vs Prospect: Binary Decision Architecture Comparison

## Quick Comparison Table

| Aspect | ProAssist | Prospect (Our) |
|--------|-----------|----------------|
| **Binary Output Shape** | `[batch, seq_len, 1]` | `[batch, 1, 1]` |
| **Granularity** | Per-token | Per-turn |
| **Head Architecture** | Linear or MLP | Linear |
| **Loss Function** | BCE with Logits | BCE with Logits |
| **Context Source** | Last position hidden state | Last position hidden state (per-turn) |
| **KV Cache** | Used during inference | Not yet optimized (full context each time) |
| **Frame Masking** | `pos_frame_mask`, `neg_frame_mask` | Binary labels per turn |
| **Training Loss** | Masked per-token BCE | Per-turn BCE |
| **Inference Strategy** | Token-by-token with cache | Turn-by-turn (can optimize) |

---

## Architecture Comparison

### ProAssist: Per-Token Binary Classification

```
Input Sequence: [img, img, text_1, text_2, ..., text_n]
                 ↓
Transformer Self-Attention (full context)
                 ↓
Hidden States: [h_0, h_1, ..., h_n]  (each h_i contains context from 0 to i)
                 ↓
Binary Head (applied to all positions)
                 ↓
Logits: [logit_0, logit_1, ..., logit_n]  shape: [batch, seq_len, 1]
                 ↓
Frame Masking: Apply pos_frame_mask and neg_frame_mask
                 ↓
BCE Loss: Per-token binary classification
```

**Example Loss Computation**:
```python
# Get logits at positions marked as "should talk"
w2t_logits_pos = outputs.w2t_logits[pos_frame_mask]  # [num_pos, 1]
labels_pos = torch.zeros_like(w2t_logits_pos)        # Target: 0
loss_pos = bce_loss(w2t_logits_pos, labels_pos)      # Per-position loss

# Get logits at positions marked as "should not talk"
w2t_logits_neg = outputs.w2t_logits[neg_frame_mask]  # [num_neg, 1]
labels_neg = torch.ones_like(w2t_logits_neg)         # Target: 1
loss_neg = bce_loss(w2t_logits_neg, labels_neg)      # Per-position loss

# Average
w2t_loss = (loss_neg + loss_pos) / 2
```

**Inference**:
```python
# At inference time, use last position's decision
not_talk_prob = outputs.w2t_probs[0, -1]  # [scalar]

# Make binary decision
if not_talk_prob > threshold:
    # Don't generate more text
else:
    # Continue generating
```

### Prospect: Per-Turn Binary Classification

```
Input Sequence: [img, img, text_1, text_2, ..., text_n]
                 ↓
Transformer Self-Attention (full context)
                 ↓
Hidden States: [h_0, h_1, ..., h_n]  (each contains context from 0 to i)
                 ↓
Extract Last Valid Position: h_last = h_n  (contains full context)
                 ↓
Reshape to Per-Turn: [h_last]  → [batch, 1, hidden_size]
                 ↓
Binary Head (applied once per turn)
                 ↓
Logits: [logit]  shape: [batch, 1, 1]
                 ↓
Binary Labels (per turn): [0 or 1]  shape: [batch]
                 ↓
BCE Loss: Per-turn binary classification
```

**Loss Computation**:
```python
# Extract context-aware embeddings (from last position)
frame_embeddings = self._extract_frame_embeddings(...)  # [batch, 1, hidden_size]

# Apply binary heads
speaking_logits = self.speaking_decision_head(frame_embeddings)  # [batch, 1, 1]
dst_logits = self.dst_update_head(frame_embeddings)             # [batch, 1, 1]

# Compute losses
speaking_loss = bce_loss(speaking_logits.squeeze(-1), speaking_labels)
dst_loss = bce_loss(dst_logits.squeeze(-1), dst_labels)
```

**Inference**:
```python
# At inference time, use the single per-turn decision
speaking_prob = torch.sigmoid(outputs.speaking_logits[0, 0, 0])  # [scalar]

if speaking_prob > threshold:
    # Generate response
else:
    # Stay silent
```

---

## Context Understanding: The Same Principle

Both implementations use the **same underlying principle** for context:

```
CONTEXT SOURCE: The hidden state at the last valid position

Why it works:
1. Transformer uses self-attention on the full input sequence
2. Each position's hidden state is the result of attending to ALL prior positions
3. The last position's hidden state is an attention-weighted combination of:
   - All image tokens
   - All prior text tokens
   - Position embeddings capturing sequence order
```

### Example: How Context Flows

```
Conversation so far:
User: "What's in the image?"
Assistant: "I see a person"
User: "Can you help?"

Tokenized: [IMG] [IMG] user_tokens assistant_tokens user_tokens_2

Forward Pass:
Position 0 (IMG):     Attention to [self]                    → Limited context
Position 1 (IMG):     Attention to [self, prev_img]          → Multimodal start
Position 2-6 (User1): Attention to [all_images, self, prev_text]
Position 7-10 (Asst): Attention to [all_images, all_user, self, prev_asst]
Position 11-13 (User2): Attention to [all_images, all_prev_text, self]  ← FULL CONTEXT

Binary Decision at Position 13:
- ProAssist: Uses h_13 (contains full context) to decide "speak" at position 13
- Prospect: Uses h_13 (contains full context) to decide "speak" for this turn

Both have the same context! Just different output format.
```

---

## KV Cache: Inference Optimization

### ProAssist's Streaming Strategy

During inference, ProAssist uses KV cache to avoid recomputing attention:

```python
past_key_values = None

for i in range(max_length):
    # First iteration: Process [IMG, IMG, text_input, assistant_output_so_far]
    # KV cache is None, so full computation
    
    # Second iteration: Process only the new token
    # KV cache contains keys/values for all previous positions
    # Only need to compute attention between new token and all cached positions
    # This is O(n) instead of O(n²)
    
    outputs = self.forward(
        inputs_embeds=inputs_embeds,
        past_key_values=past_key_values,  # Use cached k/v from previous steps
        use_cache=True,                    # Generate new k/v for this step
        return_dict=True,
    )
    past_key_values = outputs.past_key_values
```

**Benefit**: After the first token, each subsequent token only requires attention with O(n) cached keys/values instead of recomputing full self-attention.

### Current Prospect Implementation

```python
# Currently, we process full sequence each time
# Could be optimized with KV cache like ProAssist
```

---

## Which Approach is Better?

### Use ProAssist's Per-Token Approach If:

✅ **You have frame-level annotations**
- "At second 2.5, the agent should speak"
- "At image frame 5, should DST be updated"
- Multiple frames with different supervision

✅ **You want token-level granularity**
- Each token decision is independent
- Fine-grained control over when to speak

✅ **You have frame masking capability**
- `pos_frame_mask`: "These frames are when we should talk"
- `neg_frame_mask`: "These frames are when we should be silent"

✅ **You need per-position supervision**
- Different positions in the sequence have different ground truth labels

### Use Prospect's Per-Turn Approach If:

✅ **You have turn-level annotations**
- "In this turn, agent should/shouldn't speak"
- "In this turn, DST should/shouldn't be updated"
- One label per dialogue turn

✅ **You want simpler loss computation**
- Single label per turn
- No need for frame-level masking logic

✅ **Your task is turn-based dialogue**
- Natural conversational structure
- One decision per user-agent exchange

✅ **You want simpler implementation**
- Fewer moving parts
- Easier to debug

---

## Optimization Opportunity: Add KV Cache to Prospect

While both approaches produce the same context awareness during training (full sequence, transformer self-attention), the inference phase can be optimized:

**Current Prospect Inference**:
```python
# Each token generation processes full context
for i in range(max_length):
    outputs = self.forward(inputs_embeds=...)  # Recomputes attention for all positions
    # Extract speaking decision
    # Generate next token
```

**Optimized with KV Cache** (like ProAssist):
```python
past_key_values = None

for i in range(max_length):
    outputs = self.forward(
        inputs_embeds=inputs_embeds,
        past_key_values=past_key_values,  # Cached attention
        use_cache=True,                    # Cache new attention
    )
    past_key_values = outputs.past_key_values
    # Extract speaking decision from last position (still has full context via cache)
    # Generate next token
```

**Benefit**: Inference speed improvement, especially for long conversations.

---

## Summary

### Fundamental Similarity
Both Prospect and ProAssist:
- ✅ Use last position's hidden state for context
- ✅ This hidden state contains attention-weighted information from full sequence
- ✅ Apply binary heads to this context
- ✅ Use BCE loss for training
- ✅ Make binary decisions at inference time

### Key Difference
- **ProAssist**: Per-token output `[batch, seq_len, 1]` with frame masking
- **Prospect**: Per-turn output `[batch, 1, 1]` with simpler labels

### Recommendation
**Current Prospect implementation is correct and aligned with ProAssist's principle.**

The per-turn approach is appropriate if:
1. Your data has turn-level binary labels (not frame-level)
2. You prefer simpler loss computation
3. Your task is conversational turn-based dialogue

If you want to match ProAssist's inference efficiency, optionally add KV cache accumulation to `fast_greedy_generate_with_dst()`.

---

## Code Reference: Where to Find Each Component

### ProAssist
- **Model**: `/u/siddique-d1/adib/ProAssist/mmassist/model/modeling_proact.py`
  - Lines 30-42: Binary head initialization
  - Lines 256-262: Binary head application
  - Lines 124-177: `fast_greedy_generate()` with KV cache
  - Lines 296-313: Binary loss computation

### Prospect
- **Model**: `/u/siddique-d1/adib/ProAssist/custom/src/prospect/models/dst_smolvlm_with_strategies.py`
  - Lines 57-60: Binary head initialization
  - Lines 128-181: Context extraction method
  - Lines 222-252: Binary head application
  - Lines 276-303: Binary loss computation
  - Lines 340-349: Return outputs

