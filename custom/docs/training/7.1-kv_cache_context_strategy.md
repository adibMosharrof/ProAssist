# KV Cache and Context Strategy: ProAssist Reference Implementation

## Overview

This document explains how ProAssist handles context for binary decision heads (speaking/w2t decisions) and how our implementation aligns with their approach using KV cache.

---

## ProAssist's Approach

### 1. KV Cache Usage in Inference (`fast_greedy_generate`)

**Purpose**: Accumulate full conversation context efficiently during streaming generation.

**Key Points**:
- **One-pass processing per turn**: Each forward pass generates one token with `use_cache=True`
- **KV Cache accumulation**: `past_key_values` grows across token generation steps
- **Full context preserved**: All previously processed tokens remain in cache
- **Context flow**: Images + text history → full self-attention context

**Code Structure**:
```python
for i in range(max_length):
    outputs = self.forward(
        inputs_embeds=inputs_embeds,
        past_key_values=past_key_values,  # Accumulated context
        use_cache=True,                    # Cache new k/v
        return_dict=True,
    )
    past_key_values = outputs.past_key_values  # Update cache
    
    # Speaking decision from last token's hidden state
    not_talk_prob = outputs.w2t_probs[0, -1]  # Uses full context
```

**Context Source**: The hidden state at position `-1` (last token) contains full self-attention representation from all prior tokens.

### 2. Binary Decision Head Architecture

```python
# Linear variant
self.binary_decision_head = nn.Linear(hidden_size, 1)  # Output: [batch, seq_len, 1]

# Or MLP variant
self.binary_decision_head = nn.Sequential(
    nn.Linear(hidden_size, hidden_size // 2),
    nn.GELU(),
    nn.Linear(hidden_size // 2, 1),
)
```

**Output per Token**: `w2t_logits = [batch, seq_len, 1]`

### 3. Inference-Time Speaking Decision

```python
# Extract speaking probability from last position (contains full context)
not_talk_prob = outputs.w2t_probs[0, -1]  # [scalar]

# Decision logic
if not_talk_prob > threshold:
    # Silent action: inject high logit for "not_to_talk" token
    outputs.logits[:, -1, not_talk_id] = 1e4
else:
    # Continue speaking: let normal logits guide next token
    outputs.logits[:, -1, not_talk_id] = -1e4
```

**Key Insight**: The speaking decision uses the hidden state at the last position, which has seen ALL prior context through transformer self-attention.

### 4. Training-Time Loss Computation

```python
# Per-token binary classification with frame masking
w2t_logits_neg = outputs.w2t_logits[neg_frame_mask]  # "Should not talk" frames
labels_neg = torch.ones_like(w2t_logits_neg)          # Target: 1
w2t_loss_neg = bce_loss(w2t_logits_neg, labels_neg)

w2t_logits_pos = outputs.w2t_logits[pos_frame_mask]  # "Should talk" frames
labels_pos = torch.zeros_like(w2t_logits_pos)          # Target: 0
w2t_loss_pos = bce_loss(w2t_logits_pos, labels_pos)

w2t_loss = (w2t_loss_neg + w2t_loss_pos) / 2
```

**Design Pattern**: 
- Per-position binary classification (token-level)
- BCE loss with logits
- Positive/negative frame masks guide which positions contribute to loss
- Context for each position: All prior tokens + current token

---

## Our Implementation Strategy

### 1. Context Extraction

We use the **same principle** as ProAssist but adapted for per-turn outputs:

```python
def _extract_frame_embeddings(self, last_hidden_state, padding_mask):
    """
    Extract context-aware embeddings from the last valid position.
    
    The last valid token's embedding contains full self-attention context
    from all prior image and text tokens in the sequence.
    """
    batch_size = last_hidden_state.shape[0]
    
    # Find last non-padding position for each sample
    if padding_mask is None:
        last_valid_pos = last_hidden_state.shape[1] - 1
        frame_embeddings = last_hidden_state[:, last_valid_pos:last_valid_pos+1, :]
    else:
        # For padded sequences
        valid_positions = padding_mask.sum(dim=1) - 1
        frame_embeddings = last_hidden_state[
            range(batch_size), valid_positions
        ].unsqueeze(1)  # [batch, 1, hidden_size]
    
    return frame_embeddings  # [batch, 1, hidden_size]
```

**Context Similarity**: Both approaches use the last valid token's embedding, which contains full accumulated self-attention context.

### 2. Binary Head Output (Per-Turn)

```python
# Extract context-aware embeddings: [batch, 1, hidden_size]
frame_embeddings = self._extract_frame_embeddings(outputs.last_hidden_state, padding_mask)

# Apply binary heads
speaking_logits = self.speaking_decision_head(frame_embeddings)      # [batch, 1, 1]
dst_update_logits = self.dst_update_head(frame_embeddings)           # [batch, 1, 1]
```

**Design Difference**: 
- ProAssist: Per-token predictions `[batch, seq_len, 1]` during training, uses position `-1` at inference
- Ours: Per-turn predictions `[batch, 1, 1]` (single frame embedding from position `-1`)

**Equivalence**: Both use the same context (last position embedding), just different aggregation strategy for training.

### 3. Binary Loss Computation

```python
# Both decisions use identical pattern
speaking_binary_loss = nn.BCEWithLogitsLoss(reduction='mean')
dst_binary_loss = nn.BCEWithLogitsLoss(reduction='mean')

# Compute losses
speaking_loss_value = speaking_binary_loss(
    speaking_logits.squeeze(-1),  # [batch, 1] → [batch]
    speaking_labels               # [batch]
)

dst_loss_value = dst_binary_loss(
    dst_update_logits.squeeze(-1), # [batch, 1] → [batch]
    dst_update_labels              # [batch]
)

# Combined loss
total_loss = (
    response_gen_weight * response_gen_loss +
    speaking_binary_weight * speaking_loss_value +
    dst_binary_weight * dst_loss_value +
    dst_gen_weight * dst_gen_loss
)
```

**Alignment**: 
- Both use BCE with logits
- Both extract single per-turn/position decision
- Both combine with other losses
- Both are trainable end-to-end

---

## Key Insights from ProAssist

### 1. KV Cache is for Inference Efficiency, Not Training Context

**Important**: ProAssist uses KV cache primarily during generation (`fast_greedy_generate`) for efficient streaming inference. During training (`self.training=False`), KV cache is not accumulated:

```python
outputs = super().forward(
    ...
    use_cache=use_cache if not self.training else False,  # No cache during training
    ...
)
```

**Implication**: The context awareness comes from **the input sequence itself** (image + text history), not from KV cache accumulation during training.

### 2. Context is Implicit in Hidden States

Both implementations rely on the same mechanism:
- **Transformer self-attention** processes the entire input sequence at once during training
- Each position's hidden state contains attention-weighted information from all other positions
- For speaking decisions, we use the **last position's embedding** (contains all prior context)

### 3. Two Strategies for Binary Decisions

**ProAssist**: Per-token predictions `[batch, seq_len, 1]`
- Advantage: Frame-level granularity during training
- Advantage: Can use frame-level masking (pos_frame_mask, neg_frame_mask)
- Used for: Token-level supervision

**Our Approach**: Per-turn predictions `[batch, 1, 1]`
- Advantage: Cleaner for turn-based dialogue (single decision per turn)
- Advantage: Simpler loss computation
- Used for: Turn-level supervision

**Both are valid** depending on your data annotation format.

---

## Alignment with Our Implementation

### ✅ What We Got Right

1. **Context Source**: Using last token's embedding (contains full self-attention context) ✓
2. **Binary Head Design**: Single output dimension `[batch, 1, 1]` ✓
3. **Loss Type**: BCE with logits for binary decisions ✓
4. **Multi-task Learning**: Combining multiple losses ✓
5. **Identical Patterns**: Speaking and DST update follow same architecture ✓

### ⚠️ Considerations

1. **Training vs Inference**:
   - During training: Context from full input sequence via transformer attention
   - During inference: Could use KV cache for streaming context (like ProAssist)
   - **Current**: Our `fast_greedy_generate_with_dst()` can be optimized

2. **Granularity Difference**:
   - ProAssist: Per-token predictions (more granular)
   - Ours: Per-turn predictions (higher level)
   - **Choose based on**: Your data annotations (token-level or turn-level labels?)

3. **Frame Masking**:
   - ProAssist: Uses `pos_frame_mask` and `neg_frame_mask` for supervision
   - Ours: Single binary labels per turn
   - **Consider**: Do you have per-frame or per-turn annotations?

---

## Recommendations

### 1. Keep Current Approach If:
- Your data has **turn-level binary labels** (one label per conversation turn)
- You prefer **simpler loss computation**
- The model sees **full sequence during training** (not streaming)

### 2. Adopt ProAssist's Per-Token Approach If:
- Your data has **frame-level binary labels** (one label per visual frame)
- You want **token-level granularity** for decisions
- You need **frame masking** to handle mixed supervision

### 3. Optimize Inference (Optional)

To match ProAssist's streaming efficiency:
```python
# Current: Uses full context each time
# Optimization: Use KV cache accumulation like ProAssist

@torch.no_grad()
def fast_greedy_generate_with_dst(self, ...):
    past_key_values = None  # Start empty
    
    for i in range(max_length):
        outputs = self.forward(
            inputs_embeds=inputs_embeds,
            past_key_values=past_key_values,  # Accumulate
            use_cache=True,                    # Enable caching
            return_dict=True,
        )
        past_key_values = outputs.past_key_values
        
        # Speaking decision from accumulated context
        speaking_prob = torch.sigmoid(outputs.speaking_logits[0, -1, -1])
        ...
```

---

## Summary

**ProAssist's Key Strategy**:
- KV cache for **inference efficiency** (streaming generation)
- Context from **last position's hidden state** (contains full self-attention)
- Per-token **binary classification** with frame masking
- **BCE loss** for binary decisions

**Our Alignment**:
- ✅ Using last position's hidden state for context (same principle)
- ✅ Binary heads with single output `[batch, 1, 1]` (simpler variant)
- ✅ BCE loss for binary decisions (same)
- ✅ Multi-task learning combining all losses (same pattern)

**Key Difference**:
- Per-token (ProAssist) vs Per-turn (Ours) for binary output
- Both are valid; choose based on your data annotation format

**Optimization Opportunity**:
- Can optionally add KV cache accumulation to `fast_greedy_generate_with_dst()` for streaming efficiency matching ProAssist
