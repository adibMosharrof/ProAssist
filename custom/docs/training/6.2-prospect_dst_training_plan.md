# ðŸŽ¯ PROSPECT DST Training Implementation Plan

**Date:** November 21, 2025  
**Objective:** Implement DST-enhanced training for ProAssist, replacing progress summaries with structured dialogue state tracking

---

## ðŸ“‹ Executive Summary

This document outlines the complete implementation plan for training a vision-language model (SmolVLM2) using DST (Dialogue State Tracking) instead of ProAssist's original progress summaries. The implementation follows ProAssist's proven patterns while introducing structured state tracking for improved task coordination.

**Key Changes:**
- **Replace:** Progress summaries â†’ DST state
- **Add:** Binary "should_update_dst" head (similar to "when_to_speak")
- **Add:** DST transition generation (structured output: `S1->start`)
- **Maintain:** All ProAssist training patterns, evaluation metrics, and architecture

---

## ðŸŽ¯ Requirements Recap

### Data Format
- **Input:** JSON files (`train_training.json`, `val_training.json`, `test_training.json`)
- **Location:** `custom/outputs/dst_generated/hybrid_dst/2025-11-21/00-30-33_gpt-4o_proassist_50rows/assembly101/`
- **Structure:** Each JSON entry = one conversation clip with DST annotations

### Key Design Decisions
1. **DST State Format:** Structured text (`S1:completed, S2:in_progress, S3:not_started`)
2. **DST Update Format:** Minimal tokens (`S1->start` or `S2->complete`)
3. **Frame Loading:** On-demand from arrow files (memory-efficient)
4. **Training Pattern:** Follow ProAssist exactly (turn-by-turn processing)
5. **Evaluation:** Validation loss only (no dialogue generation during training)
6. **Negative Sampling:** 50% of silent frames (fixed, configurable for future curriculum learning)
7. **Loss Weights:** Follow ProAssist pattern (initially equal, configurable)

---

## ðŸ“Š Phase 1: Data Loading Implementation

### 1.1 Files to Update
- `custom/src/prospect/data_sources/dst_training_dataset.py`
- `custom/src/prospect/data_sources/dst_training_datamodule.py`

### 1.2 Dataset Class Architecture

```python
class DSTTrainingDataset(Dataset):
    """
    Dataset for DST-enhanced ProAssist training
    
    Key Features:
    - Load clips from JSON files (each clip is one training sample)
    - On-demand frame loading from arrow files
    - DST state context in system prompt
    - Turn-by-turn processing with conversation history
    """
    
    def __init__(self, 
                 data_path: str,          # Path to train/val/test JSON
                 dataset_name: str,       # 'assembly101', etc.
                 processor: AutoProcessor,
                 max_seq_len: int = 4096,
                 neg_frame_sampling_rate: float = 0.5):
        pass
    
    def __len__(self):
        return len(self.clips)  # Each clip is one sample
    
    def __getitem__(self, idx):
        """
        Returns one training sample (one clip with all its turns)
        
        Returns:
            {
                'video_uid': str,
                'clip_idx': int,
                'frames_file': str,
                'conversation': List[Dict],  # All turns in this clip
                'dst': List[Dict],           # Global DST steps
                'system_prompt': str,        # With DST steps included
                'turns_data': List[Dict],    # Processed turn data
            }
        """
        pass
```

### 1.3 Frame Loading Strategy

**Implementation: On-demand loading (Option A)**

```python
def load_frames_for_turn(self, frames_file: str, start_frame: int, end_frame: int):
    """
    Load frames on-demand from arrow file for current turn
    
    Args:
        frames_file: Path to arrow file (e.g., 'data/assembly101/frames/video.arrow')
        start_frame: Start frame index (e.g., 193)
        end_frame: End frame index (e.g., 195)
    
    Returns:
        List of PIL Images or tensors (preprocessed)
    
    Note: Follow ProAssist preprocessing exactly:
        - Resize to model's expected size
        - Normalize using processor
        - Convert to tensors
    """
    import pyarrow as pa
    
    # Load arrow file
    arrow_table = pa.ipc.open_file(frames_file).read_all()
    
    # Extract frame range
    frames = arrow_table['images'][start_frame:end_frame+1]
    
    # Apply ProAssist preprocessing
    processed_frames = self.processor.preprocess_images(frames)
    
    return processed_frames
```

**Rationale:**
- GPU memory constraint: 24GB
- Large number of frames across all videos
- Memory-efficient: only load what's needed for current batch
- Trade-off: Slightly slower I/O, but avoids OOM

### 1.4 System Prompt Construction

```python
def build_system_prompt(self, dst_steps: List[Dict], initial_dst_state: Dict = None) -> str:
    """
    Build system prompt with DST steps (replaces task knowledge)
    
    Format:
        You are a proactive assistant helping with task completion.
        
        Task Steps:
        - S1: Assemble the chassis by attaching and screwing the chassis parts together
        - S2: Attach wheels to the chassis
        - S3: Assemble the arm and attach it to the chassis
        ...
        
        [For split conversations, add current state:]
        Current Progress: S1:completed, S2:in_progress
    
    Args:
        dst_steps: Global DST array from JSON
        initial_dst_state: For split conversations (clip_idx > 0)
    """
    prompt_parts = [
        "You are a proactive assistant helping with task completion.",
        "",
        "Task Steps:"
    ]
    
    # Add DST steps (id + name, omit timestamps)
    for step in dst_steps:
        prompt_parts.append(f"- {step['id']}: {step['name']}")
    
    # For split conversations, add current progress
    if initial_dst_state:
        state_str = ", ".join([f"{k}:{v}" for k, v in initial_dst_state.items()])
        prompt_parts.append("")
        prompt_parts.append(f"Current Progress: {state_str}")
    
    return "\n".join(prompt_parts)
```

### 1.5 Turn-by-Turn Processing

**Follow ProAssist Pattern:**

```python
def process_clip_turns(self, clip_data: Dict) -> List[Dict]:
    """
    Process each turn in the clip individually (ProAssist pattern)
    
    For each turn:
    1. Load frames for that turn [start_frame, end_frame]
    2. Build conversation history up to current turn
    3. Determine if turn is:
       - assistant: compute language modeling loss
       - DST_UPDATE: compute DST binary + generation loss
       - user: include in context, no loss
       - system: include in context, no loss
    4. Apply negative sampling for silent frames
    
    Returns:
        List of processed turns ready for batching
    """
    conversation = clip_data['conversation']
    frames_file = clip_data['frames_file']
    
    processed_turns = []
    
    for turn_idx, turn in enumerate(conversation):
        # Load frames for this turn
        frames = self.load_frames_for_turn(
            frames_file, 
            turn['start_frame'], 
            turn['end_frame']
        )
        
        # Build conversation history (all previous turns)
        history = conversation[:turn_idx]
        
        # Determine turn type and loss computation
        turn_type = turn['role']
        compute_loss = self._should_compute_loss(turn, turn_idx)
        
        # Apply negative sampling for silent frames
        if self._is_silent_frame(turn):
            if random.random() > self.neg_frame_sampling_rate:
                continue  # Skip this silent frame
        
        processed_turns.append({
            'turn': turn,
            'frames': frames,
            'history': history,
            'compute_loss': compute_loss,
            'turn_type': turn_type,
        })
    
    return processed_turns
```

### 1.6 Silent Frame Detection

```python
def _is_silent_frame(self, turn: Dict) -> bool:
    """
    Identify silent frames (no assistant or DST_UPDATE)
    
    Silent frames: Model should not speak or update DST
    Active frames: assistant or DST_UPDATE turns
    
    Args:
        turn: Conversation turn dict
    
    Returns:
        True if silent (user turn with no model action)
    """
    return turn['role'] not in ['assistant', 'DST_UPDATE']
```

### 1.7 Data Collator

```python
class DSTDataCollator:
    """
    Collate batch of turns with frames + text
    
    Handles:
    - Variable-length conversations
    - Frame tensors batching
    - Text tokenization
    - DST labels preparation
    """
    
    def __call__(self, batch: List[Dict]) -> Dict[str, torch.Tensor]:
        """
        Collate batch of processed turns
        
        Returns:
            {
                'input_ids': Tensor,           # Tokenized text
                'attention_mask': Tensor,
                'pixel_values': Tensor,        # Frames
                'labels': Tensor,              # For language modeling loss
                'dst_binary_labels': Tensor,   # For should_update_dst
                'dst_gen_labels': Tensor,      # For DST generation
                'loss_mask': Tensor,           # Which turns to compute loss on
            }
        """
        pass
```

---

## ðŸ—ï¸ Phase 2: Model Architecture Updates

### 2.1 Files to Update
- `custom/src/prospect/models/dst_smolvlm_with_strategies.py`

### 2.2 Model Architecture

```python
class DSTSmolVLMWithStrategies(SmolVLM2ForConditionalGeneration):
    """
    SmolVLM2 with DST prediction heads
    
    Architecture:
    - Base: SmolVLM2ForConditionalGeneration
    - Add: should_update_dst binary head (like when_to_speak)
    - Add: DST generation capability (structured output)
    
    Input: frames + conversation text
    Output: 
        - Language modeling logits (assistant responses)
        - should_update_dst prediction (binary)
        - DST transition generation (when should_update_dst=True)
    """
    
    def __init__(self, config):
        super().__init__(config)
        
        # DST binary head (similar to when_to_speak)
        self.dst_binary_head = nn.Linear(
            config.text_config.hidden_size,
            2  # [no_update, update]
        )
        
        # Initialize weights
        self.dst_binary_head.apply(self._init_weights)
    
    def forward(
        self,
        input_ids,
        attention_mask,
        pixel_values,
        labels=None,
        dst_binary_labels=None,
        dst_gen_labels=None,
        loss_mask=None,
        **kwargs
    ):
        """
        Forward pass with DST prediction
        
        Returns:
            {
                'loss': combined loss,
                'lm_loss': language modeling loss,
                'dst_binary_loss': should_update_dst loss,
                'dst_gen_loss': DST generation loss,
                'logits': language modeling logits,
                'dst_binary_logits': should_update_dst logits,
            }
        """
        pass
```

### 2.3 DST Binary Head (Similar to when_to_speak)

```python
def compute_dst_binary_prediction(self, hidden_states, attention_mask):
    """
    Predict should_update_dst from frame token embeddings
    
    Args:
        hidden_states: Model hidden states
        attention_mask: Attention mask
    
    Returns:
        dst_binary_logits: [batch_size, 2] (no_update vs update)
    
    Note: Operates on frame token embeddings, similar to ProAssist's when_to_speak
    """
    # Extract frame token embeddings (last layer)
    # Follow ProAssist pattern exactly
    
    frame_embeddings = self.extract_frame_embeddings(hidden_states, attention_mask)
    
    # Binary classification
    dst_binary_logits = self.dst_binary_head(frame_embeddings)
    
    return dst_binary_logits
```

### 2.4 DST Generation Logic

```python
def generate_dst_update(self, input_ids, attention_mask, pixel_values):
    """
    Generate DST update transition (only when should_update_dst=True)
    
    Format: S1->start or S2->complete
    
    Args:
        input_ids, attention_mask, pixel_values: Model inputs
    
    Returns:
        generated_dst_update: str (e.g., "S1->start")
    
    Note: 
    - Only called when dst_binary_head predicts "update"
    - Uses standard generation (like assistant responses)
    - Constrained to DST format
    """
    # Use model's generate() method
    output_ids = self.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        pixel_values=pixel_values,
        max_new_tokens=20,  # DST updates are short
    )
    
    # Decode and parse
    dst_update = self.decode_dst_update(output_ids)
    
    return dst_update
```

---

## ðŸŽ¯ Phase 3: Loss Computation

### 3.1 Multi-Task Loss

```python
def compute_loss(
    self,
    lm_logits,
    dst_binary_logits,
    labels,
    dst_binary_labels,
    dst_gen_labels,
    loss_mask,
):
    """
    Compute combined multi-task loss
    
    Loss Components:
    1. Language Modeling Loss (assistant turns only)
    2. DST Binary Loss (all active frames + 50% silent frames)
    3. DST Generation Loss (DST_UPDATE turns only)
    
    Args:
        lm_logits: Language modeling logits
        dst_binary_logits: should_update_dst logits
        labels: Ground truth tokens for language modeling
        dst_binary_labels: Ground truth for should_update_dst [0=no, 1=yes]
        dst_gen_labels: Ground truth DST updates (tokenized)
        loss_mask: Which turns to compute loss on
    
    Returns:
        {
            'total_loss': weighted sum of all losses,
            'lm_loss': language modeling loss,
            'dst_binary_loss': should_update_dst loss,
            'dst_gen_loss': DST generation loss,
        }
    """
    losses = {}
    
    # 1. Language Modeling Loss (only on assistant turns)
    lm_loss = self.compute_lm_loss(lm_logits, labels, loss_mask)
    losses['lm_loss'] = lm_loss
    
    # 2. DST Binary Loss (with negative sampling)
    dst_binary_loss = self.compute_dst_binary_loss(
        dst_binary_logits, 
        dst_binary_labels, 
        loss_mask
    )
    losses['dst_binary_loss'] = dst_binary_loss
    
    # 3. DST Generation Loss (only on DST_UPDATE turns)
    dst_gen_loss = self.compute_dst_gen_loss(
        lm_logits,  # Reuse language modeling head
        dst_gen_labels, 
        loss_mask
    )
    losses['dst_gen_loss'] = dst_gen_loss
    
    # Weighted combination (follow ProAssist pattern)
    total_loss = (
        self.lm_loss_weight * lm_loss +
        self.dst_binary_weight * dst_binary_loss +
        self.dst_gen_weight * dst_gen_loss
    )
    losses['total_loss'] = total_loss
    
    return losses
```

### 3.2 Loss Computation Details

**Language Modeling Loss:**
```python
def compute_lm_loss(self, logits, labels, loss_mask):
    """
    Compute on assistant turns only
    Skip: user, system, DST_UPDATE turns
    """
    # Standard cross-entropy loss
    # Apply loss_mask to only compute on assistant turns
    pass
```

**DST Binary Loss:**
```python
def compute_dst_binary_loss(self, logits, labels, loss_mask):
    """
    Compute on:
    - All assistant turns (label=0, no DST update)
    - All DST_UPDATE turns (label=1, update)
    - 50% of silent frames (label=0, negative samples)
    
    Follow ProAssist's when_to_speak pattern exactly
    """
    # Binary cross-entropy loss
    # Apply negative sampling for silent frames
    pass
```

**DST Generation Loss:**
```python
def compute_dst_gen_loss(self, logits, labels, loss_mask):
    """
    Compute only on DST_UPDATE turns
    
    Ground truth: tokenized DST update (e.g., "S1->start")
    Prediction: model's generated tokens
    """
    # Standard cross-entropy loss
    # Only compute on turns with role='DST_UPDATE'
    pass
```

### 3.3 Loss Weighting Strategy

**Initial Weights (Follow ProAssist):**
```yaml
loss_weights:
  lm_loss_weight: 1.0        # Language modeling (baseline)
  dst_binary_weight: 1.0     # DST binary (same as when_to_speak)
  dst_gen_weight: 1.0        # DST generation
  
# Make configurable for later tuning
# Can adjust based on validation performance
```

---

## ðŸ‹ï¸ Phase 4: Training Loop

### 4.1 Files to Update
- `custom/src/prospect/train/dst_training_prospect.py`

### 4.2 Training Configuration

```python
@dataclass
class TrainingConfig:
    """Training hyperparameters"""
    
    # Model
    model_name: str = "HuggingFaceTB/SmolVLM2-2.2B-Instruct"
    
    # Data
    data_path: str = "custom/outputs/dst_generated/hybrid_dst/2025-11-21/00-30-33_gpt-4o_proassist_50rows"
    dataset_name: str = "assembly101"
    max_seq_len: int = 4096
    
    # Training
    num_epochs: int = 10
    batch_size: int = 2
    gradient_accumulation_steps: int = 8
    learning_rate: float = 1e-5
    weight_decay: float = 0.01
    warmup_steps: int = 100
    
    # Loss weights
    lm_loss_weight: float = 1.0
    dst_binary_weight: float = 1.0
    dst_gen_weight: float = 1.0
    
    # Negative sampling
    neg_frame_sampling_rate: float = 0.5  # Fixed for now, make adaptive later
    
    # Optimization
    gradient_checkpointing: bool = True   # Memory efficiency
    bf16: bool = True                     # Mixed precision training
    
    # Checkpointing
    save_total_limit: int = 5             # Keep last 5 checkpoints
    save_steps: int = 500
    
    # Evaluation
    eval_steps: int = 500                 # Validation every 500 steps
    eval_strategy: str = "steps"
    
    # Logging
    logging_steps: int = 10
    report_to: str = "tensorboard"
```

### 4.3 Training Loop Implementation

```python
class DSTTrainer:
    """
    DST-enhanced training following ProAssist pattern
    
    Key Features:
    - Turn-by-turn processing
    - Multi-task loss (LM + DST binary + DST generation)
    - Negative frame sampling
    - Gradient checkpointing
    - Checkpoint saving with optimizer state
    """
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.setup_model()
        self.setup_data()
        self.setup_optimizer()
        self.setup_logging()
    
    def setup_model(self):
        """Initialize model with DST heads"""
        self.model = DSTSmolVLMWithStrategies.from_pretrained(
            self.config.model_name
        )
        
        # Enable gradient checkpointing
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
    
    def setup_data(self):
        """Load train/val datasets"""
        self.train_dataset = DSTTrainingDataset(
            data_path=f"{self.config.data_path}/{self.config.dataset_name}/train_training.json",
            dataset_name=self.config.dataset_name,
            processor=self.processor,
            max_seq_len=self.config.max_seq_len,
            neg_frame_sampling_rate=self.config.neg_frame_sampling_rate,
        )
        
        self.val_dataset = DSTTrainingDataset(
            data_path=f"{self.config.data_path}/{self.config.dataset_name}/val_training.json",
            dataset_name=self.config.dataset_name,
            processor=self.processor,
            max_seq_len=self.config.max_seq_len,
            neg_frame_sampling_rate=1.0,  # No negative sampling for validation
        )
    
    def train(self):
        """Main training loop"""
        # Use HuggingFace Trainer for standard loop
        trainer = Trainer(
            model=self.model,
            args=self.training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            data_collator=self.data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        # Train from scratch (no checkpoint resume)
        trainer.train()
        
        # Save final model
        trainer.save_model()
        self.processor.save_pretrained(trainer.args.output_dir)
```

### 4.4 Validation Loop

```python
def validate(self):
    """
    Validation: Compute loss only (Option A - fast)
    
    No dialogue generation during training
    Just compute validation losses:
    - LM loss
    - DST binary loss
    - DST generation loss
    
    Note: Full evaluation with dialogue generation will be separate
    """
    self.model.eval()
    
    total_lm_loss = 0
    total_dst_binary_loss = 0
    total_dst_gen_loss = 0
    
    with torch.no_grad():
        for batch in self.val_dataloader:
            outputs = self.model(**batch)
            
            total_lm_loss += outputs['lm_loss'].item()
            total_dst_binary_loss += outputs['dst_binary_loss'].item()
            total_dst_gen_loss += outputs['dst_gen_loss'].item()
    
    # Log metrics
    self.log_validation_metrics({
        'val/lm_loss': total_lm_loss / len(self.val_dataloader),
        'val/dst_binary_loss': total_dst_binary_loss / len(self.val_dataloader),
        'val/dst_gen_loss': total_dst_gen_loss / len(self.val_dataloader),
    })
```

---

## ðŸ“Š Phase 5: Evaluation Integration

### 5.1 Evaluation Strategy

**During Training:**
- **Validation every N steps** (e.g., 500 steps)
- **Compute loss only** (fast, no dialogue generation)
- **Track:** LM loss, DST binary loss, DST generation loss

**Post-Training (Separate Script):**
- **Full evaluation** with dialogue generation
- **Call ProAssist metrics** directly (don't reimplement)
- **Track:**
  - When-to-speak: Precision, Recall, F1
  - Dialogue quality: BLEU, METEOR, CIDEr
  - DST-specific: should_update_dst F1, DST update accuracy

### 5.2 Metrics Integration

```python
def compute_metrics(self, eval_preds):
    """
    Compute metrics during validation
    
    Metrics:
    1. Perplexity (language modeling)
    2. DST binary accuracy/F1
    3. DST generation accuracy
    
    Note: Full ProAssist metrics (BLEU, etc.) computed post-training
    """
    predictions, labels = eval_preds
    
    # 1. Perplexity
    perplexity = compute_perplexity(predictions, labels)
    
    # 2. DST binary F1
    dst_binary_f1 = compute_binary_f1(
        predictions['dst_binary_preds'],
        labels['dst_binary_labels']
    )
    
    # 3. DST generation accuracy (exact match)
    dst_gen_accuracy = compute_dst_accuracy(
        predictions['dst_gen_preds'],
        labels['dst_gen_labels']
    )
    
    return {
        'perplexity': perplexity,
        'dst_binary_f1': dst_binary_f1,
        'dst_gen_accuracy': dst_gen_accuracy,
    }
```

### 5.3 ProAssist Evaluation Integration

```python
# Post-training evaluation script
# Call ProAssist evaluator directly

from prospect.prospect_evaluator import ProspectEvaluator

evaluator = ProspectEvaluator(
    model_path="path/to/checkpoint",
    data_source="proassist_dst",
    video_ids=["9011-c03f"],  # Validation videos
)

# Run evaluation (generates dialogues + computes metrics)
results = evaluator.evaluate()

# Results will include:
# - when_to_speak_f1
# - dialogue_bleu
# - dialogue_meteor
# - dialogue_cider
# - dst_update_f1 (new metric)
# - dst_accuracy (new metric)
```

---

## ðŸ” Outstanding Questions for User

### Critical Questions (Need answers before implementation):

1. **DST State Context in Conversation**
   - Each turn has `dst_state`. Should I:
     - **A:** Use as input context (prepend to turn text)?
     - **B:** Only use for loss computation (compare predicted vs actual)?
     - **C:** Both (context + supervision)?

2. **ProAssist Frame Handling**
   - When `start_frame=193, end_frame=195`, do they:
     - **A:** Use all 3 frames [193, 194, 195]?
     - **B:** Sample subset (e.g., just middle frame 194)?
     - **C:** Use for negative sampling?

3. **DST Generation Target - Multiple Updates**
   - Format for single update: `S1->start` âœ“
   - Format for multiple simultaneous updates:
     - **A:** `S1->start, S2->start`
     - **B:** Generate separately
     - **C:** Not possible in current data?

4. **Conversation History Context**
   - For turn N, should model see:
     - **A:** All previous turns [0...N-1] in that clip?
     - **B:** Sliding window (last K turns)?
     - **C:** What does ProAssist do?

5. **Initial DST State for Split Conversations**
   - For clip_idx > 0, should first turn's system prompt include:
     - **A:** "Current Progress: S1:completed, S2:in_progress"
     - **B:** Just the DST steps (model infers from history)
     - **C:** Both?

---

## ðŸ“ Implementation Checklist

### Phase 1: Data Loading âœ…
- [ ] Update `dst_training_dataset.py`
  - [ ] Load JSON files (train/val/test)
  - [ ] On-demand frame loading from arrow files
  - [ ] System prompt with DST steps
  - [ ] Turn-by-turn processing
  - [ ] Negative frame sampling
- [ ] Update `dst_training_datamodule.py`
  - [ ] Data collator for batching
  - [ ] Handle variable-length conversations

### Phase 2: Model Architecture âœ…
- [ ] Update `dst_smolvlm_with_strategies.py`
  - [ ] Add `dst_binary_head` (like when_to_speak)
  - [ ] DST generation logic
  - [ ] Forward pass with all outputs

### Phase 3: Loss Computation âœ…
- [ ] Implement multi-task loss
  - [ ] Language modeling loss (assistant only)
  - [ ] DST binary loss (with negative sampling)
  - [ ] DST generation loss (DST_UPDATE only)
  - [ ] Weighted combination

### Phase 4: Training Loop âœ…
- [ ] Update `dst_training_prospect.py`
  - [ ] Training configuration
  - [ ] Trainer setup with gradient checkpointing
  - [ ] Checkpoint saving (last 5)
  - [ ] Validation every N steps (loss only)

### Phase 5: Evaluation âœ…
- [ ] Validation metrics during training
- [ ] Post-training evaluation script
- [ ] ProAssist metrics integration

---

## ðŸŽ¯ Success Criteria

1. **Training runs successfully** end-to-end
2. **Validation loss decreases** over epochs
3. **DST binary F1 > 0.8** (should_update_dst prediction)
4. **DST generation accuracy > 0.7** (correct transitions)
5. **Language modeling perplexity comparable** to ProAssist baseline
6. **Model saves checkpoints** properly (last 5 + optimizer state)
7. **Evaluation metrics match** ProAssist format

---

## ðŸ“š References

- **ProAssist Paper:** `custom/papers/proassist.pdf`
- **ProAssist Code:** `mmassist/train`, `mmassist/eval`, `mmassist/model`
- **Sample Data:** `custom/docs/training/6.1-prospect_training_data.json`
- **Training Plan Doc:** `custom/docs/training/6-simplified_dst_training_proassist_pattern.md`
- **Implementation Plan:** `custom/docs/training/3-dst_training_implementation_plan.md`
- **Existing Model:** `custom/src/prospect/models/dst_smolvlm_with_strategies.py`
- **Existing Dataset:** `custom/src/prospect/data_sources/dst_training_dataset.py`
- **Frame Loading Reference:** `custom/src/prospect/runners/vlm_stream_runner.py`

---

## ðŸš€ Next Steps

1. **User answers outstanding questions** (Section above)
2. **Start Phase 1:** Update data loading code
3. **Test data loading** with small batch
4. **Proceed to Phase 2-5** sequentially
5. **Run training** on assembly101 dataset
6. **Evaluate** and iterate

---

**End of Implementation Plan**
