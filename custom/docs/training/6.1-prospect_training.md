
custom/docs/training/6-simplified_dst_training_proassist_pattern.md

the above file gives some idea of how I plan to train.



custom/docs/training/6.1-prospect_training_data.json

the above file is a sample data with dst information in it.


custom/src/prospect/train/dst_training_prospect.py

the above file was an attempt to start training.

this file has was written with a different processed data.

now I have to adapt it to use style of data in the json I had mentioned above.

I dont want backward compatibility to the existing features.

please update this file.

I am extending the proassist functionality.

I am replacing the progress summary with DST. 
Every turn will have the current DST state, which will replace the progress summary.
For every item in the conversation array, all the necessary information is contained in that item.
The whole DST is provided in a separate key in the data, you should use that instead of the task knowledge.
The DST will provide information on what things to do and in what order.

This will be multimodal training, I will have both image and text as input to the model.


## Task

You job will be to write code to start training the model using the DST information present in the processed data.


## Questions About the Training Plan

### Round 1

1. Data Structure & DST Integration
In 6.1-prospect_training_data.json, each conversation turn has a dst_state field. Should I use this per-turn DST state, or should I also reference the global dst array for additional context?

- The global dst can be added in the task description.

The dst_state in each turn contains completed_steps, current_step, current_substep, current_action, and next_step. Should all of these be incorporated into the system prompt for that turn?

in the current dst implementation, i have skipped substep and action.
the dst does not have a depth of 3, it only has a depth of 1.
so you will only incorporate completed_steps, current_step, next step.

2 Image/Frame Handling
Each turn has a frame_idx field. Should I load the actual frame image from the video dataset and include it in the model input?

yes the actual image should be loaded and used in the model input.

How should frames be loaded? Should I use the existing ProAssist video dataset infrastructure, or do you have a different approach in mind?
Should I include multiple recent frames or just the current frame at each turn?

for images, we will follow exactly what proassist did.
mmassist/train
the folder above has more details on the training details of proassist.

3. Training Objective
Based on your plan, I understand the model should learn to generate appropriate dialogue responses given:
Current DST state
Conversation history
Current frame(s)
Is the training objective to predict the assistant response given all the previous context?
Should I use the standard causal language modeling loss, or do you want any special loss weighting?

the doc below has plans on how i want to create my model and my training plan.
the data processing has changed, so please look at the json file in custom/docs/training/6.1-prospect_training_data.json to see what data you will have to use.
custom/docs/training/3-dst_training_implementation_plan.md
the doc above should have details about the model, loss and other details.


4. System Prompt Construction

I would like to pass in the global DST as well in the task instruction.
Look at the prompt used by proassist. A lot of things will be the same.
I will replace their progress summary with the current state of the dst.
For the task instruction, I will replace the task knowledge with the full dst.

6. Dataset Preparation

custom/outputs/dst_generated/hybrid_dst/2025-11-21/00-30-33_gpt-4o_proassist_50rows/

the above folder is where you will find the data.


I will have multiple datasets, see the folders under

data/proassist/processed_data

to see what datasets I will have.

Right now I am only working with assembly101, but I will incorporate the other datasets as well.

custom/outputs/dst_generated/hybrid_dst/2025-11-21/00-30-33_gpt-4o_proassist_50rows/assembly101

you see that there are multiple json files.

you will use the train_training.json, val_training.json and test_training.json files.

The data splits are already created.

7. Conversation History
How much conversation history should be included in each turn's input?
Should I include all previous turns, or just the last N turns to fit within context limits?

I have a token limit, I have prepared the data to be within the token limit, so you should not have to worry about data not being able to fit in the context.
I have split conversations into multple clips, similar to how proassist did it.
Please take a look at their code to better understand how they are handling this.

8. Output Format
Should the trained model be saved in a specific format?
Do you need checkpoints at specific intervals?
Should I integrate with the existing PROSPECT evaluation pipeline?

just save the checkpoint so that I can load it for inference.
just save the last 5 checkpoints.
yes, you have to integrate with existing prospect evaluation pipeline.

### Round 2.

Questions About DST Structure & Implementation
1. DST State Representation

Looking at the JSON, I see the global dst array has entries like:
{
  "type": "STEP",
  "id": "1",
  "start_ts": 97.2,
  "end_ts": 116.5,
  "name": "Assemble the chassis..."
}

And each turn has dst_state like:

{
  "completed_steps": ["1", "2"],
  "current_step": "3",
  "current_substep": "3.1",
  "current_action": "3.1.1",
  "next_step": "4"
}

no that is the incorrect dst state representation.

see custom/docs/training/6.1-prospect_training_data.json for the proper dst structure.

2. DST Transition Prediction
You mention "DST Transition Prediction" as one of the losses.

Questions:

What constitutes a "transition"? Is it when current_step changes from one turn to the next?
Should I predict the next DST state given the current state + frames + conversation?
Or should I predict when a transition will occur (e.g., predict if the current step will complete in the next N frames)?

yes a transition is when the status of a step changes, like from not started to in progress or from in progress to completed.

you will have to predict when a DST update event will happen, and also what should change.

3. Frame Loading & Video Access
The JSON has frame_idx fields, but no explicit video path.

the data has a frames file path.

```json
"frames_file": "data/assembly101/frames/assembly_nusar-2021_action_both_9011-c03f_9011_user_id_2021-02-01_160239__HMC_84355350_mono10bit.arrow",
```

you will have to load the frames from there.

look at the custom/src/prospect/runners/vlm_stream_runner.py to see how I loaded frames from the arrow file.

there will be a lot of images, so preprocessing/caching might not be feasible.


4. DST_UPDATE Events
In the sample JSON, I don't see explicit "DST_UPDATE" role entries in the conversation array.

Questions:

Should I infer DST_UPDATE events by comparing dst_state between consecutive turns?
Or will the actual training data have explicit DST_UPDATE entries?
If I need to create them: Should every turn with a different dst_state than the previous turn get a DST_UPDATE event?

are you looking at the wrong json?

you should look at the json file in the path below.
custom/docs/training/6.1-prospect_training_data.json

I have added a snippet from that file below. It has the explicit dst_update role entries.
```json
{
    "role": "DST_UPDATE",
    "content": [
        {
            "id": "S1",
            "transition": "start"
        }
    ],
    "time": 97.2,
    "start_frame": 193,
    "end_frame": 195,
    "dst_state": {
        "S1": "not_started"
    },
    "labels": "initiative|dst_update,dst_start,steps_1"
}
```

5. Multi-turn Conversation Splits
You mention "When conversations are split due to token limits, DST state ensures continuity."

Questions:

What's the max token limit per training sample?
When splitting, should I:
End the split at the last complete assistant turn?
Or can I split mid-conversation?
Should each split start with a system message containing the initial DST state for that segment?

I used 4096 as the max token limit.
After a split, the DST context will be there in the first turn.
You dont have to split, the data has already been split.
A video is broken down into rows in the json file, you have to look at the clip index.

6. Training Loss Weighting
You have 3 loss components:

Language Modeling Loss (assistant content)
DST State Classification
DST Transition Prediction
Questions:

Should these losses have equal weight, or should I weight them differently?
Do you have target weights in mind (e.g., 1.0 for LM, 0.5 for DST state, 0.3 for transitions)?

Yes you can try out different weights. For now I just want to start the training. We can optimize the weights later.

7. DST Heads Architecture
You mention "Add DST heads to ProAssist model."

Questions:

Should the DST heads be:
Simple linear classifiers on top of frame token embeddings?
More complex (e.g., multi-layer MLPs)?
Should there be separate heads for:
Step prediction
Substep prediction
Action prediction
Or one unified head that predicts the full hierarchical state?

See how proassist modelled their when to speak feature.
mmassist/model
you can look in the folder above.

I will do the same here, I will first have to decide whether there will be a DST update event in this frame.
Then we will generate the DST update.



8. Negative Frame Sampling
You mention neg_frame_sampling_rate = 0.5 for silent frames.

Questions:

How do I identify "silent" frames? Frames where there's no assistant turn?
Should negative sampling apply to DST state classification loss as well, or only language modeling?
For frames with DST_UPDATE events, should they always be included (sampling rate = 1.0)?

Silent frames are when we have no assistant turn or dst update events, basically where the model will not have to do anything.
Yes negative sampling should apply to DST state classification as well.
Sample the DST update events in the same way you would sample the when to speak feature.


9. Model Input Format
Questions:

Should the DST state be encoded as:
Natural language text prepended to the assistant turn (e.g., "Current: Step 3, Substep 3.1, Action 3.1.1")?
Special tokens (e.g., <DST_STEP_3><DST_SUBSTEP_3.1>)?
Or left implicit (model learns from the conversation flow)?
How should I represent the global dst array in the model input? Should it be part of the initial system prompt?


i am still open to how I want to represent the dst state. I will have to predict the state as well and then compare it against the ground truth, so suggest a few options that will allow the model to learn how to predict and at the same time will allow me to evaluate and see whether the model made the correct state prediction.

the global dst array should be part of the initial system prompt.

10. Evaluation Metrics
Questions:

What metrics should I track during training?
Language modeling perplexity?
DST state accuracy (exact match or hierarchical accuracy)?
Transition prediction accuracy?
Should I implement any validation loops, or just training loss monitoring?

for evaluation, I want to follow proassist exactly.
I want to report the same metrics as they are doing.
I ran on a single video and reported results.
custom/src/prospect/prospect_evaluator.py

you can see what i did in the file above.
you should take a look at how proassist is doing evaluation, I want to report the same metrics as I will compare against their performance.
you can find the evaluation code of proassist in the folder mmassist/eval.

11. Existing Code Compatibility
Questions:

You mentioned "I dont want backward compatibility to the existing features." Does this mean I should:
Completely rewrite dst_training_prospect.py from scratch?
Or can I keep the general structure (data loading, training loop) and just change the data format handling?
Should the trained model be compatible with the existing PROSPECT inference/evaluation code, or is this a separate training pipeline?

Keep the existing structure. Just change the data format handling. You dont have to add backward compatiblity to the previous structure. I will drop the previous data structure altogether.

We can worry about inference later, I would like it to be compatible with the existing pipeline if possible.

### Round 3

1. DST State Representation in Model Input/Output
Looking at the JSON, the dst_state for each turn is:

"dst_state": {
    "S1": "not_started",
    "S2": "not_started", 
    "S3": "not_started",
    ...
}

And DST_UPDATE events have:

{
    "role": "DST_UPDATE",
    "content": [
        {"id": "S1", "transition": "start"}
    ]
}

Questions:

For DST state prediction, should the model output be:

Option A: A classification for each step independently? (e.g., predict "S1: in_progress", "S2: not_started" separately)
Option B: A single multi-label classification? (e.g., predict entire state dict at once)
Option C: Natural language generation? (e.g., generate "S1 is in progress, S2 not started...")


answer:
natural language generation. for different conversations and domains, we dont know how many steps there will be.
so NLG is the only natural solution.


For DST transition prediction, should the model:

Option A: Predict binary "will there be a transition in this frame?" + classify which step + classify transition type (start/complete)?
Option B: Predict a structured output like {"id": "S1", "transition": "start"} directly?
Option C: Generate natural language that gets parsed?

answer: similar to should speak, you have a head for should update dst.
the model has to make a binary decision on that head. then for generating the output, the model has to generate the structured output.


How should I encode DST state in the system prompt? For example:

Task Steps:
- S1: Assemble chassis [Status: not_started]
- S2: Attach wheels [Status: not_started]
...

Is this format acceptable, or do you prefer a different structure?

answer: This format is good, as long as we can parse it later and calculate metrics properly.

2. DST Update Event Prediction Details
You said "I will first have to decide whether there will be a DST update event in this frame."

Questions:

Should this be modeled as:

A binary classifier per frame (DST_UPDATE: yes/no)?
Similar to ProAssist's "when_to_speak" head?
If there's a DST_UPDATE predicted, how should the model generate the actual update content [{"id": "S1", "transition": "start"}]?
Separate classification heads for step_id and transition_type?
Sequence generation?
Multiple binary classifiers (one per step, one per transition type)?

answer: yes, you have to do it similar to proassist's when to speak head.
the model should generate the updated content. we will compare against the dst state from the json data.

3. Global DST in System Prompt
The global dst array has full step details:

"dst": [
    {
        "id": "S1",
        "type": "STEP",
        "start_ts": 97.2,
        "end_ts": 116.5,
        "name": "Assemble the chassis by attaching and screwing the chassis parts together"
    },
    ...
]

Questions:

Should I include all fields (id, type, timestamps, name) in the system prompt?
Or just id + name (omit timestamps)?

Assembly Steps:
1. S1: Assemble the chassis by attaching and screwing the chassis parts together
2. S2: Attach wheels to the chassis
...

Or a different format?

answer: only name and id, omit the timestamps.

4. Frame Loading Implementation
You mentioned looking at vlm_stream_runner.py for frame loading from arrow files.

Questions:

Should I load frames the exact same way (using pyarrow to read from .arrow files)?
For training, should I:
Load all frames for a video into memory at the start of an epoch?
Load frames on-demand for each batch?
Use some caching strategy?
What preprocessing should be applied to frames (resize, normalize, etc.)? Should I follow ProAssist exactly?

answer: follow proassist exactly. in the data, they have frame numbers tied to different events, so see what they do with regards to loading frames.
in your plan please mention how to plan to solve it.

5. Model Architecture Clarification
You said "See how proassist modelled their when to speak feature."

Questions:

Should I extend the existing ProAssist model by adding:

A DST_UPDATE detection head (similar to when_to_speak)?
DST state prediction heads (one per step for status classification)?
DST transition prediction heads (step_id classifier + transition_type classifier)?
Should these heads operate on:

Frame token embeddings (like when_to_speak)?
Last hidden state?
Some combination?

answer: i have a version of model created.

custom/src/prospect/models/dst_smolvlm_with_strategies.py

take a look at that. update this to handle the new data we have made.
i am using smolvlm2, whereas proassist has used separate models for text and image.

6. Training Data Structure
Looking at the file structure:

custom/outputs/dst_generated/hybrid_dst/2025-11-21/00-30-33_gpt-4o_proassist_50rows/assembly101/
├── train_training.json
├── val_training.json  
├── test_training.json

Questions:

Each JSON file contains an array of training samples, where each sample is one "clip" (conversation segment), correct?
The clip_index field identifies which clip of a video this is?
For a single training sample, should I:
Process all conversation turns in that clip as one forward pass?
Or process turn-by-turn (each turn is a separate training step)?

answer: see how proassist does this. i want to follow the same pattern.

7. Loss Computation Details
You have 3 loss components:

Language Modeling Loss (assistant content)
DST State Classification Loss
DST Transition Prediction Loss
Questions:

For DST State Classification:

Should I compute loss on every assistant turn that has a dst_state field?
Or only on specific turns (e.g., after DST_UPDATE events)?
For DST Transition Prediction:

Should I compute loss only on frames with DST_UPDATE events?
Should negative samples (frames without DST_UPDATE) be included with label "no_transition"?
Should all three losses be summed together, or computed separately and logged?

answer: since the dst binary decision and state generation is modeled like the when to speak and assistance utterance, I want to follow the same pattern.


8. Negative Frame Sampling Implementation
Questions:

For negative frame sampling:

Should I randomly sample 50% of "silent" frames to include in training?
Or should I include all frames but only compute loss on 50% of silent frames?
When a frame is sampled as negative:

Should I still compute language modeling loss if there's text?
Should I compute DST state classification loss?
Should I skip all losses for that frame?

answer: see what proassist is doing for when to speak. i want a similar pattern for the dst as well.


9. Evaluation Integration
You want to follow ProAssist evaluation exactly and report same metrics.

Questions:

Should I implement evaluation during training (validation loop after each epoch)?
Or should evaluation be a separate script that loads checkpoints?
What specific metrics from ProAssist should I track:
When-to-speak precision/recall/F1?
BLEU/METEOR/CIDEr for dialogue quality?
Should I add DST-specific metrics (state accuracy, transition accuracy)?

answer: look at how i got metrics in the vlm_stream_runner.py.
custom/src/prospect/runners/vlm_stream_runner.py
i dont want to reimplement a lot of things. if possible i want to call the metrics from proassist so that the numbers are comparabale.


10. Data Format Validation
Before I start implementation, can you confirm:

All training data will be in the format shown in 6.1-prospect_training_data.json?
Each sample will have:
video_id
clip_index
frames_file (path to arrow file)
dst (global array)
conversation (array of turns with dst_state and frame info)
initial_dst_state (for conversation continuity)
Are there any other fields I should expect or handle?

answer: yes the data will be in that format.

11. Implementation Priority
Given the scope of work, what should be the implementation priority?

Get basic training loop working with language modeling loss only?
Add DST state classification?
Add DST transition prediction?
Add evaluation?
Or should I implement everything together from the start?

answer: implement everything together from the start.


### Round 4

1. DST State Generation Format
You mentioned using natural language generation (NLG) for DST state prediction. Could you clarify the exact format you want the model to generate?

go with a structured text

 S1: in_progress, S2: not_started, S3: not_started

this will be easier to parse. i dont want to go for json as the model will have to generate valid json and it might struggle to do that.

2. DST Update Generation Format
For DST_UPDATE events, should the model generate:

go with a structured format like below.

DST_UPDATE: S1 -> start


3. Training Sample Construction
Looking at the JSON, each video can have multiple clips (conversation segments). For training:

Should each clip be treated as an independent training sample?
When processing a clip's conversation turns, should I:
Feed all turns sequentially in one forward pass (like a full conversation)?
Or process turn-by-turn where each turn gets its own forward pass?
ProAssist seems to do turn-by-turn processing with frame context. Should I follow the same pattern?

answer: yes follow proassists pattern.

4. Frame Selection Strategy
Each conversation turn has start_frame and end_frame. Should I:

Option A: Use all frames in the range [start_frame, end_frame]?
Option B: Sample frames from the range (e.g., first, middle, last)?
Option C: Use only the middle/representative frame?

What did ProAssist do, and should I follow exactly?
answer: yes do exactly what proassit did.

5. DST Binary Head vs State Generation
You mentioned having a binary "should_update_dst" head (like "when_to_speak"). Then:

If should_update_dst = True, should the model:

Generate the full DST state for that turn?
Or only generate the transition (which step changed and how)?
answer: the data has the transition, so its better to only generate the transition.
it will be less things to generate. the problem with generating the full dst is that the model might not properly generate the full dst.

If should_update_dst = False, should the model still:

Generate the current DST state for context tracking?
Or skip DST generation entirely?
answer: lets skip dst generation


6. Loss Computation Timing
For each conversation turn, should I compute:

Language Modeling Loss:

Only on assistant role turns?
Also on user role turns?
Skip on DST_UPDATE turns?
DST State Loss:

On every turn (to maintain state tracking)?
Only on assistant and DST_UPDATE turns?
DST Update Binary Loss:

On every frame?
Only on frames with activity (assistant or DST_UPDATE)?


answer: see how proassist did things with when to speak and the assistant turn generation.
i want to have a similar pattern.

7. Initial DST State Handling
The JSON has initial_dst_state for conversation continuity. Should this:

Be prepended to the first turn's system prompt?
Be used to initialize model's internal state?
Be included in loss computation (compare predicted vs initial)?

answer: include it in the first turn's system prompt.


8. Multi-turn Context Window
Within a clip's conversation:

Should the model see all previous turns in that clip as context?
Or should I implement a sliding window (e.g., last N turns)?
Does the 4096 token limit already account for this?

answer: see what proassist did, i want to do the same.


9. Checkpoint and Resumption
For training:

Should I implement gradient checkpointing for memory efficiency?
Do you want to resume from any existing ProAssist checkpoint, or train from scratch?
Should I save optimizer state for proper resumption?

answer: i want to train from scratch. yes add gradient checkpointing. yes save optimizer state.


10. Validation Loop Implementation
You mentioned following ProAssist evaluation exactly. Should I:

Run evaluation after every epoch?
Run evaluation every N steps?
Only run at the end of training?
Generate actual dialogues on validation videos and compute metrics?

answer: run eval after N steps. did proassist generate actual dialogs on validation videos?
i think this might make training significantly slower. if it only adds a small overhead, then we can do it.


### Round 5

1. ProAssist Evaluation During Training
You asked: "did proassist generate actual dialogs on validation videos?"

Let me check their training code to see what they did. Should I:

Option A: Just compute validation loss (fast, no dialogue generation)
Option B: Generate dialogues on a small subset of validation videos (e.g., 5-10 videos) for qualitative inspection
Option C: Full evaluation with dialogue generation on all validation videos
What's your preference for balancing training speed vs. evaluation thoroughness?

answer: stick with option A.

2. Existing Data Source Files
I see these files in data_sources:

dst_training_dataset.py - Dataset class
dst_training_datamodule.py - DataModule wrapper
Other dataset files
Should I:

Completely replace the content of these files (no backward compatibility)?
Or create new files (e.g., dst_training_dataset_v2.py) and keep old ones for reference?

answer: completely replace with no backward compatibility. but look at the coding pattern. i want to have a similar style.

3. DST State in System Prompt - Per Turn or Once?
For DST state context in each turn, should I:

Option A: Include FULL DST state in every turn's context (e.g., "S1: completed, S2: in_progress, S3: not_started")
Option B: Include DST state only in system prompt at the start, then rely on DST_UPDATE events to track changes
Option C: Include only CHANGED steps in each turn (e.g., only mention S2 if it changed)

answer: no dont give full dst state at every turn. we already have the full dst as the initial system prompt.
that turn will be in the dialog history.
i dont know what the difference is between option b and c. follow the pattern of the input data i have for the dst update.
i want to hae a similar structure.

4. Frame Loading - Caching Strategy
Since you mentioned "there will be a lot of images, so preprocessing/caching might not be feasible", should I:

Option A: Load frames on-demand for each batch from arrow files (slower but memory-efficient)
Option B: Pre-load all frames for current epoch into shared memory (faster but memory-intensive)
Option C: Load and cache frames for current batch + next N batches (balanced approach)
What did ProAssist do, and what's your GPU memory constraint?

answer: i have 24gb gpu. option B and C might be an overkill. stick with option A.

5. DST Generation Token Format

answer: use a format that is easy to parse and does not need a lot of tokens. also i want an easy format so that the model can only focus on producing the right dst update and not have to worry about the structure of the output.

6. Loss Weighting Initial Values
You mentioned we can optimize weights later. For initial training, should I use:

Equal weights: lm_loss_weight=1.0, dst_binary_weight=1.0, dst_gen_weight=1.0
ProAssist-style weights: (if they had weights for when_to_speak vs. language modeling)
Your suggestion: Any initial weights you'd like to try?

answer: follow proassist style weights. I dont have a preference on the weights right now.

7. Negative Frame Sampling Rate
You mentioned neg_frame_sampling_rate = 0.5. Should this be:

Fixed at 0.5 for all silent frames?
Configurable via config file?
Adaptive (e.g., start at 0.5, increase to 1.0 during training)?

answer: for now keep it fixed at 0.5, but i will want to make it adaptive later on.
I want to add curriculum learning to this, so as the number of epochs/steps increases, i will change the ratio.

8. Data Fields to Expect
Looking at the actual JSON files in your output folder vs. the sample, should I expect all these fields?

{
    "video_uid": str,
    "clip_idx": int,
    "frames_file": str,
    "start_frame_idx": int,
    "end_frame_idx": int,
    "dst": List[Dict],  # Global DST
    "conversation": List[Dict],  # Turns with dst_state, start_frame, end_frame
    "inferred_goal": str,
    "inferred_knowledge": str,
    "metadata": Dict,
    "seq_len": int,
    "max_seq_len": int,
    # Any others?
}

in the sample, I have reduced a lot of the extra data that will be present in the json file from the output data.
those are some extra data that we can later use for debugging. right now focus on the items present in the data.