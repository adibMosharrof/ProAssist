# Window-Based Two-Stage Decision Architecture for Prospect

## Overview

Transform Prospect from per-turn binary decisions to a **window-based two-stage architecture** that:
1. **Stage 1 (Coarse)**: Decides should we speak/update DST in a window?
2. **Stage 2 (Fine)**: Pinpoints exactly which frame triggered the decision (±2 frame tolerance)

This approach leverages **chronological interleaving** of images and text to provide rich multimodal context for decisions grounded in specific video moments.

---

## Architecture Design

### Core Concept

```
Video Timeline: |----4s----|----4s----|----4s----|----4s----|
                  Window 1    Window 2    Window 3    Window 4
                  (0-4s)      (4-8s)      (8-12s)     (12-16s)

Each Window Contains:
├── Images: [frame_0, frame_1, ..., frame_8] (at 2fps = 8 frames per 4s)
├── Text: User messages, DST transitions within window timespan
└── Context: Accumulated DST state, prior conversation

Model Processes:
1. Chronological interleave of images + text by timestamp
2. Stage 1: Binary decisions (speak? dst_update?) at [<DECIDE>] token
3. Stage 2: Frame pinpointing (which frame triggered it?)
```

### Training Example

**Data Clip** (from your test_training.json):
```json
{
  "video_uid": "assembly_9012-...",
  "conversation": [
    {"role": "system", "content": "You are a proactive assistant..."},
    {"role": "user", "time": 159.3, "start_frame": 316, "end_frame": 320, 
     "content": "I want to assemble a toy clamp..."},
    {"role": "assistant", "time": 159.3, "start_frame": 316, "end_frame": 320,
     "content": "Great goal! To start, let's attach the step..."},
    {"role": "DST_UPDATE", "time": 159.3, "start_frame": 316, "end_frame": 320,
     "content": [{"id": "S1", "transition": "start"}]},
    ...more turns...
  ],
  "fps": 2,
  "frames_file": "..."
}
```

**Extract Windows** (4s window, 2s stride):

Window 1: [0-4s] → frames 0-7
- No events → speak=0, dst_update=0 (negative window)

Window 2: [2-6s] → frames 4-11
- User speaks at 3.3s (frames 6-7)
- Assistant responds (frames 6-7)
- DST updates (frames 6-7)
- → speak=1, dst_update=1, trigger_frames=[6,7]

Window 3: [4-8s] → frames 8-15
- Assistant responds at ~4.2s
- → speak=1, dst_update=0

---

## Implementation Components

### 1. Window Extraction Module

**File**: `custom/src/prospect/data_sources/window_extractor.py`

**Functionality**:
- Input: Full clip JSON (1-3 minute conversation, 1000+ tokens)
- Output: N windows × [window_data with chronological sequence]

**Key Methods**:
```python
class WindowExtractor:
    def extract_windows_from_clip(
        self, 
        clip: Dict,
        window_size_seconds: float = 4.0,
        window_stride_seconds: float = 2.0,
        context_mode: str = "last_turns"  # or "summary"
    ) -> List[Dict]:
        """
        Extract overlapping windows from a clip.
        
        Returns: List of window samples ready for training
        """
        
    def build_chronological_sequence(
        self,
        frames: List[int],
        text_events: List[Dict],
        window_start_time: float,
        window_end_time: float
    ) -> List[Union[str, torch.Tensor]]:
        """
        Build [<CONTEXT>] [IMG] [IMG] [TEXT@ts] [IMG] ... [<DECIDE>]
        
        Interleave frames and text by timestamp.
        """
        
    def create_window_labels(
        self,
        conversation_turns: List[Dict],
        frames: List[int],
        window_start_time: float,
        window_end_time: float
    ) -> Dict[str, Any]:
        """
        Generate:
        - should_speak: binary
        - should_dst_update: binary
        - trigger_frame_indices: which frames caused decisions
        - trigger_tolerance: ±2 frames is correct
        """
```

**Algorithm**:
1. Parse clip's conversation and extract (timestamp, frame, text) tuples
2. Slide window across video:
   - For each window [t, t+4s]:
     - Collect frames in range
     - Collect text events in range
     - Chronologically interleave by timestamp
     - Create binary labels (speak? dst_update?)
     - Identify trigger frames
3. Split into positive (has events) and negative (no events) windows
4. Apply 1:1 or 1:2 positive:negative ratio

**Data Flow**:
```
Clip JSON (seq_len=1716)
    ↓
Extract windows (window_size=4s, stride=2s)
    ↓
For each window:
    - Extract frames 0-8 (if 0-4s window)
    - Extract text events in time range
    - Build chronological sequence
    - Create labels
    ↓
Window samples (seq_len=~200-400 each)
```

---

### 2. Chronological Sequence Builder

**Purpose**: Interleave images and text by timestamp (Option C from earlier discussion)

**Structure**:
```
[<CONTEXT>]
├── DST Knowledge: "1. Assemble chassis... 2. Attach wheels..."
├── Current DST State: "{S1: 'not_started', S2: 'not_started'}"
└── Prior Turns Summary: "User wants to assemble toy. Assistant explained process."

[<FRAMES>]
├── [IMG_316] → frame 0
├── [IMG_317] → frame 1
├── [TEXT@159.3s] "I want to assemble a toy..."  (user message at frame ~2)
├── [IMG_318] → frame 2
├── [IMG_319] → frame 3
├── [TEXT@159.3s] "Great! Let's attach the step..." (assistant at frame ~2)
└── [IMG_320] → frame 4

[<DECIDE>]  ← Binary heads applied here
└── Attention can see: all frames + all text + context
```

**Text Insertion Logic**:
- User/DST messages: insert at frame closest to their timestamp
- If message timestamp between frames, insert at earlier frame with timestamp annotation
- Format: `[TIMESTAMP:159.3s] [ROLE:user] I want to assemble...`

---

### 3. Data Collator Modifications

**File**: `custom/src/prospect/data_sources/dst_data_collator.py`

**Changes**:
- Input: List of window samples (variable seq_len 200-400)
- Collate into batch with padding
- Create masks for per-frame supervision

**New Inputs**:
```python
batch = {
    # Standard text/image inputs
    "input_ids": torch.Tensor,
    "attention_mask": torch.Tensor,
    "pixel_values": torch.Tensor,  # Window frames
    
    # Window-level binary labels (Stage 1)
    "should_speak": torch.BoolTensor,  # [batch_size]
    "should_dst_update": torch.BoolTensor,  # [batch_size]
    
    # Frame-level pinpointing (Stage 2)
    "trigger_frame_indices": torch.LongTensor,  # [batch_size, num_frames]
    "trigger_tolerance": int,  # ±2 frames is acceptable
    
    # Per-frame supervision masks
    "pos_frame_mask": torch.BoolTensor,  # Frames where event happens
    "neg_frame_mask": torch.BoolTensor,  # Frames where no event
}
```

**Key Method**: `_create_frame_masks()`
- Mark which frames in window triggered decisions
- Create masks for per-frame BCE loss (like ProAssist)

---

### 4. Model Architecture Updates

**File**: `custom/src/prospect/models/dst_smolvlm_with_strategies.py`

#### Stage 1: Window-Level Binary Decision

**Input**: Full chronological sequence + `[<DECIDE>]` token
**Output**: Window-level probabilities

```python
class DSTSmolVLMWithStrategies(SmolVLMWithStrategies):
    
    def __init__(self, config):
        super().__init__(config)
        
        # Stage 1: Window-level binary heads
        self.speak_decision_head = nn.Linear(hidden_size, 1)  # [batch, 1] logit
        self.dst_decision_head = nn.Linear(hidden_size, 1)    # [batch, 1] logit
        
        # Stage 2: Frame pinpointing head
        self.frame_trigger_head = nn.Linear(hidden_size, 1)   # Per-frame logits
        
    def forward(self, input_ids, pixel_values, **kwargs):
        # Standard VLM forward
        outputs = super().forward(input_ids, pixel_values, ...)
        last_hidden_state = outputs.last_hidden_state
        
        # Extract [<DECIDE>] token embedding
        decide_token_pos = find_token_position(input_ids, "<DECIDE>")
        decide_embedding = last_hidden_state[:, decide_token_pos, :]
        
        # Stage 1: Binary decisions
        speak_logits = self.speak_decision_head(decide_embedding)      # [batch, 1]
        dst_logits = self.dst_decision_head(decide_embedding)          # [batch, 1]
        
        return {
            "speak_logits": speak_logits,
            "dst_logits": dst_logits,
            ...
        }
```

**Loss** (Stage 1):
```python
bce_loss_fn = nn.BCEWithLogitsLoss()

speak_loss = bce_loss_fn(speak_logits.squeeze(-1), should_speak.float())
dst_loss = bce_loss_fn(dst_logits.squeeze(-1), should_dst_update.float())

stage1_loss = speak_loss + dst_loss
```

#### Stage 2: Frame Pinpointing

**Input**: All frame position embeddings in window
**Output**: Per-frame logits (softmax → argmax is predicted trigger frame)

```python
def forward_frame_pinpointing(self, last_hidden_state, frame_indices):
    """
    Extract embeddings at frame positions.
    
    last_hidden_state: [batch, seq_len, hidden_size]
    frame_indices: [batch, num_frames_in_window]  e.g., [0, 1, 2, 3, 4]
    
    Returns: [batch, num_frames, 1] logits for each frame
    """
    batch_size, num_frames = frame_indices.shape
    
    # Extract frame embeddings
    frame_embeddings = []
    for b in range(batch_size):
        frame_emb = last_hidden_state[b, frame_indices[b], :]  # [num_frames, hidden]
        frame_embeddings.append(frame_emb)
    
    # Stack: [batch, num_frames, hidden]
    frame_embeddings = torch.stack(frame_embeddings)
    
    # Apply head: [batch, num_frames, 1]
    frame_logits = self.frame_trigger_head(frame_embeddings)
    
    return frame_logits
```

**Loss** (Stage 2):
```python
# Softmax over frame positions
frame_probs = F.softmax(frame_logits, dim=1)  # [batch, num_frames]

# Cross-entropy with tolerance
# If predicted frame within ±2 of ground truth → target=1, else target=0
tolerance = 2
target_mask = torch.zeros_like(frame_probs)  # [batch, num_frames]

for b in range(batch_size):
    gt_frame = trigger_frame_indices[b]
    for f in range(num_frames):
        if abs(f - gt_frame) <= tolerance:
            target_mask[b, f] = 1.0

# Soft label: distribute probability among tolerance range
target_labels = target_mask / (target_mask.sum(dim=1, keepdim=True) + 1e-8)

# BCE with soft targets
stage2_loss = -(target_labels * torch.log(frame_probs + 1e-8)).sum(dim=1).mean()
```

---

### 5. Trainer Integration

**File**: `custom/src/prospect/train/dst_custom_trainer.py`

**Changes**:
- Compute Stage 1 loss (window-level binary)
- Compute Stage 2 loss (frame pinpointing) only if should_speak=1 or should_dst_update=1
- Combine with response generation loss

**Loss Computation**:
```python
def compute_loss(self, model, inputs, return_outputs=False):
    # Forward pass
    outputs = model(**inputs)
    
    # Stage 1: Window-level binary decisions
    speak_loss = bce_loss_fn(
        outputs.speak_logits.squeeze(-1), 
        inputs["should_speak"].float()
    )
    dst_loss = bce_loss_fn(
        outputs.dst_logits.squeeze(-1),
        inputs["should_dst_update"].float()
    )
    
    # Stage 2: Frame pinpointing (only for positive windows)
    positive_mask = inputs["should_speak"] | inputs["should_dst_update"]
    if positive_mask.any():
        frame_logits = outputs.frame_logits[positive_mask]
        trigger_indices = inputs["trigger_frame_indices"][positive_mask]
        
        frame_loss = compute_pinpoint_loss(
            frame_logits, 
            trigger_indices,
            tolerance=2
        )
    else:
        frame_loss = torch.tensor(0.0, device=speak_loss.device)
    
    # Response generation loss (LM loss)
    lm_loss = outputs.loss  # From VLM forward
    
    # Combine with weights
    total_loss = (
        1.0 * speak_loss +
        1.0 * dst_loss +
        0.5 * frame_loss +
        1.0 * lm_loss
    )
    
    return total_loss
```

---

### 6. Inference Pipeline

**File**: New file `custom/src/prospect/inference/window_inference.py`

**Flow**:
```
Video Stream
    ↓
Extract 4s windows (one per second or real-time)
    ↓
For each window:
    1. Build chronological sequence
    2. Forward through model
    3. Get Stage 1 outputs (speak_prob, dst_prob)
    4. If speak_prob > threshold:
         - Get Stage 2 outputs (frame logits)
         - Find argmax frame
         - Generate response text
    5. If dst_prob > threshold:
         - Generate DST update
    6. Accumulate context (update DST state, add to conversation)
    ↓
Output: (timestamp, action, generated_text)
```

**Implementation**:
```python
@torch.no_grad()
def run_windowed_inference(self, video_frames, conversation_history, dst_state):
    """
    Args:
        video_frames: All frames from video
        conversation_history: Prior conversation
        dst_state: Current DST state
        
    Returns:
        decisions: List of (timestamp, speak, dst_update, generated_text, dst_change)
    """
    
    window_size_frames = 8  # 4s at 2fps
    stride_frames = 4       # 2s stride
    threshold_speak = 0.5
    threshold_dst = 0.5
    
    decisions = []
    
    # Slide window across video
    for window_start in range(0, len(video_frames) - window_size_frames, stride_frames):
        window_end = window_start + window_size_frames
        window_frames = video_frames[window_start:window_end]
        
        # Build chronological sequence
        sequence = build_chronological_sequence(
            window_frames,
            conversation_history,
            dst_state
        )
        
        # Tokenize and prepare inputs
        inputs = tokenizer(sequence, return_tensors="pt")
        
        # Forward pass
        with torch.no_grad():
            outputs = model(**inputs)
        
        # Stage 1: Binary decisions
        speak_prob = torch.sigmoid(outputs.speak_logits[0, 0]).item()
        dst_prob = torch.sigmoid(outputs.dst_logits[0, 0]).item()
        
        # Stage 2: Pinpointing (if decision is positive)
        if speak_prob > threshold_speak:
            frame_logits = outputs.frame_logits[0]  # [num_frames, 1]
            trigger_frame_idx = torch.argmax(frame_logits).item()
            trigger_timestamp = window_start + trigger_frame_idx / fps
            
            # Generate response
            response = model.generate_response(sequence)
            
            decisions.append({
                "timestamp": trigger_timestamp,
                "action": "speak",
                "confidence": speak_prob,
                "trigger_frame": trigger_frame_idx,
                "text": response
            })
        
        if dst_prob > threshold_dst:
            frame_logits = outputs.frame_logits[0]
            trigger_frame_idx = torch.argmax(frame_logits).item()
            trigger_timestamp = window_start + trigger_frame_idx / fps
            
            # Generate DST update
            dst_update = model.generate_dst_update(sequence)
            dst_state = update_dst_state(dst_state, dst_update)
            
            decisions.append({
                "timestamp": trigger_timestamp,
                "action": "dst_update",
                "confidence": dst_prob,
                "trigger_frame": trigger_frame_idx,
                "dst_change": dst_update
            })
    
    return decisions
```

---

## Data Transformation Requirements

### Current Data Format (ProAssist style)
```json
{
  "video_uid": "...",
  "conversation": [
    {"role": "user", "time": 159.3, "start_frame": 316, "end_frame": 320, "content": "..."},
    {"role": "assistant", "time": 159.3, "start_frame": 316, "end_frame": 320, "content": "..."},
    {"role": "DST_UPDATE", "time": 159.3, "start_frame": 316, "end_frame": 320, "content": [...]}
  ],
  "fps": 2,
  "frames_file": "...",
  "seq_len": 1716,
  "max_seq_len": 4096
}
```

### Target Window Format
```json
{
  "video_uid": "...",
  "window_id": "9012_window_0",
  "window_start_time": 0.0,
  "window_end_time": 4.0,
  "window_start_frame": 0,
  "window_end_frame": 8,
  
  "context": {
    "dst_knowledge": "1. Assemble chassis... 2. Attach wheels...",
    "current_dst_state": {"S1": "not_started", "S2": "not_started"},
    "prior_conversation": []
  },
  
  "frames": [0, 1, 2, 3, 4, 5, 6, 7],
  
  "text_events": [
    {"timestamp": 0.0, "role": "system", "content": "You are a proactive assistant..."},
    {"timestamp": 2.5, "role": "user", "content": "I want to assemble..."},
    {"timestamp": 2.5, "role": "assistant", "content": "Great! Let's attach..."},
    {"timestamp": 2.5, "role": "DST_UPDATE", "content": {"id": "S1", "transition": "start"}}
  ],
  
  "labels": {
    "should_speak": 1,
    "should_dst_update": 1,
    "trigger_frame_indices": [2, 3],
    "trigger_frames_speak": [2, 3],
    "trigger_frames_dst": [2, 3]
  },
  
  "fps": 2,
  "frames_file": "...",
  "seq_len": ~250,
  "max_seq_len": 4096
}
```

**Transformation Pipeline**:
```
Full Clip JSON
    ↓
WindowExtractor.extract_windows_from_clip()
    ↓
For each window:
    - Extract frames
    - Collect text events in time range
    - Build context from prior turns
    - Create labels
    ↓
List of Window JSONs
    ↓
Save to new JSON file
    ↓
DSTWindowDataset loads window JSONs
```

---

## Configuration

**New config file**: `custom/config/prospect/model/dst_window_smolvlm2.yaml`

```yaml
model:
  model_name: "smolvlm2-2.2b-instruct"
  hidden_size: 2048
  
  # Binary decision heads
  speak_head_type: "linear"  # or "mlp"
  dst_head_type: "linear"
  
  # Frame pinpointing head
  pinpoint_head_type: "linear"

training:
  # Window configuration
  window_size_seconds: 4.0
  window_stride_seconds: 2.0
  window_sampling: "overlapping"  # or "non-overlapping"
  
  # Context
  context_mode: "last_turns"  # or "summary"
  num_prior_turns: 5
  
  # Negative sampling
  negative_ratio: 1.0  # 1:1 positive:negative windows
  
  # Losses
  speak_weight: 1.0
  dst_weight: 1.0
  frame_pinpoint_weight: 0.5
  response_gen_weight: 1.0
  dst_gen_weight: 1.0
  
  # Frame tolerance
  trigger_tolerance: 2  # ±2 frames is correct

inference:
  window_stride_frames: 4  # 2s at 2fps
  threshold_speak: 0.5
  threshold_dst_update: 0.5
  
fps: 2
```

---

## Implementation Timeline

### Phase 1: Foundation (Future)
- [ ] Implement `WindowExtractor` class
- [ ] Create chronological sequence builder
- [ ] Update data collator for window samples

### Phase 2: Model Updates (Future)
- [ ] Add Stage 1 binary heads to model
- [ ] Add Stage 2 frame pinpointing head
- [ ] Integrate losses into trainer

### Phase 3: Data Preparation (Future)
- [ ] Transform test_training.json → window format
- [ ] Create DSTWindowDataset
- [ ] Validate window extraction

### Phase 4: Training & Inference (Future)
- [ ] Train on windowed samples
- [ ] Implement windowed inference pipeline
- [ ] Evaluate Stage 1 (binary decision accuracy) and Stage 2 (frame pinpoint error)

---

## Metrics & Evaluation

### Stage 1 Metrics
- **Speak Decision**:
  - Precision: % of predicted "speak" windows that had actual speak
  - Recall: % of actual "speak" windows that were predicted correctly
  - F1: Harmonic mean
  
- **DST Update Decision**:
  - Precision, Recall, F1 (same as above)

### Stage 2 Metrics
- **Frame Pinpointing** (only for positive windows):
  - MAE (Mean Absolute Error): Average frame offset from ground truth
  - % Correct within tolerance: % of predictions within ±2 frames
  - Median frame error

### Combined Metrics
- **End-to-End Accuracy**: (Correct Stage 1 + Correct Stage 2) / Total windows
- **Per-Action Accuracy**: Separate scores for speak vs. dst_update

---

## Comparison with ProAssist

| Aspect | ProAssist | Prospect (Window-Based) |
|--------|-----------|------------------------|
| **Decision Granularity** | Per-token (at image tokens) | Per-frame (explicit frame positions) |
| **Decision Output** | Implicit (use last image token) | Explicit (stage 1 + stage 2) |
| **Context** | Full clip (autoregressive) | Window + accumulated context |
| **Training** | Next-token prediction | Classification + pinpointing |
| **Inference** | Token-by-token generation | Window-by-window sliding |

---

## Future Enhancements

1. **Refinement with attention**: Use attention weights to explain which frames/text mattered
2. **Multi-frame triggers**: Handle decisions spanning multiple frames
3. **Hierarchical context**: Compress prior turns into hierarchical summary
4. **Real-time streaming**: Process live video without future context
5. **Hard negative mining**: Prioritize confusing negative windows during training
