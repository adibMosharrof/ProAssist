# Model Input Sanity Check: Data Flow Visualization

## Overview

This is a simple POC document to visualize and verify that the correct data is being passed into the model. We'll use ASCII diagrams similar to the training pipeline doc to show the data structure at each stage.

The key enhancement in this document is that diagrams will be **generated from actual data** using a visualization script, showing:
1. **Frame numbers** instead of generic `<image>` tokens (so you can verify correct frames)
2. **Timeline visualization** to verify temporal alignment
3. **Silence modeling** to understand how gaps between turns are represented

---

## 1. Raw Data: Conversation JSON

Starting with one sample from `train_training.json`:

```
┌───────────────────────────────────────────────────────────┐
│  Raw Conversation Sample                                  │
│                                                           │
│  video_uid: "assembly_nusar-..."                          │
│  inferred_goal: "Assembling a Toy Vehicle..."             │
│  conversation: [                                          │
│    {                                                      │
│      role: "system",                                      │
│      content: "You are a helpful assistant...",           │
│      start_frame: 0,   end_frame: 2,                      │
│      dst_state: {}                                        │
│    },                                                     │
│    {                                                      │
│      role: "DST_UPDATE",                                  │
│      content: [{id: "S1", transition: "start"}],          │
│      start_frame: 186, end_frame: 190,                    │
│      dst_state: {S1: "not_started"}                       │
│    },                                                     │
│    {                                                      │
│      role: "user",                                        │
│      content: "I want to assemble a toy vehicle...",      │
│      start_frame: 186, end_frame: 190,                    │
│      dst_state: {S1: "in_progress"}                       │
│    },                                                     │
│    {                                                      │
│      role: "assistant",                                  │
│      content: "Great! Let's get started...",              │
│      start_frame: 187, end_frame: 191,                    │
│      dst_state: {S1: "in_progress"}                       │
│    },                                                     │
│    ...more turns...                                       │
│  ],                                                       │
│  dst: [step definitions],                                │
│  total_frames: 259                                        │
└───────────────────────────────────────────────────────────┘
```

---

## 2. Loaded Sample from Dataset

After loading with precomputed embeddings:

```
┌───────────────────────────────────────────────────────────┐
│  DSTTrainingDataset.__getitem__() Output                  │
│                                                           │
│  Returns a dictionary with:                              │
│                                                           │
│  "conversation": [Turn 0, Turn 1, Turn 2, ...]           │
│  "embeddings": numpy array [259, 2048]                   │
│  "clip_id": "assembly_nusar_..."                         │
│  "sample_idx": 0                                          │
│                                                           │
│  embeddings breakdown:                                    │
│  ┌─ Frame 0:   [2048-dim vector]                          │
│  ├─ Frame 1:   [2048-dim vector]                          │
│  ├─ Frame 2:   [2048-dim vector]                          │
│  ├─ ...                                                   │
│  └─ Frame 258: [2048-dim vector]                          │
│                                                           │
│  SANITY CHECK:                                            │
│  ✓ Embeddings shape = [num_frames, 2048]                 │
│  ✓ Conversation has all turns with roles                 │
│  ✓ Each turn has frame range (start_frame, end_frame)    │
│  ✓ Frame ranges are within [0, 258]                      │
└───────────────────────────────────────────────────────────┘
```

---

## 3. Formatted Conversation with Frame Information

After `format_conversation_with_ranges()`, displayed on a **timeline**:

```
Timeline of Frames and Turns (Sample Visualization from Real Data):

Frame:    0   1   2  ... 185 186 187 188 189 190 191 ... 258
          |   |   |       |   |   |   |   |   |   |       |
Turn 1:   [S] [S] [S]                                     
          system prompt

Turn 2:                            [D] [D] [D] [D] [D]    
                                   DST_UPDATE

Turn 3:                            [U] [U] [U] [U] [U]    
                                   user text

Turn 4:                                [A] [A] [A] [A]    
                                       assistant text

┌─────────────────────────────────────────────────────────┐
│  GENERATED DIAGRAM FROM ACTUAL DATA:                    │
│  Shows:                                                 │
│  ├─ Exact frame numbers across the timeline             │
│  ├─ Which frames belong to which turn                   │
│  ├─ Frame ranges with role labels (S/D/U/A)             │
│  ├─ Gaps (silence) between turns                        │
│  └─ DST state transitions at each frame                 │
│                                                         │
│  Formatted text with frame numbers:                     │
│  <frame_0><frame_1><frame_2>You are helpful...         │
│  <frame_186><frame_187><frame_188><frame_189><frame_190>
│  [DST_UPDATE] S1: start                                 │
│  <frame_186><frame_187><frame_188><frame_189><frame_190>
│  I want to assemble...                                  │
│  <frame_187><frame_188><frame_189><frame_190><frame_191>
│  Great! Let's...                                        │
│                                                         │
│  SILENCE MODELING:                                      │
│  Gaps between turns (e.g., frames 3-185) are:          │
│  - Either included as negative sampling frames          │
│  - Or skipped entirely (sparse model)                   │
│  - Or marked with special SILENCE tokens                │
│  (depends on implementation configuration)              │
│                                                         │
│  SANITY CHECKS:                                         │
│  ✓ Frame numbers are correct and sequential             │
│  ✓ No duplicate frame numbers across turns              │
│  ✓ Frame ranges match JSON metadata                     │
│  ✓ Temporal ordering is preserved                       │
│  ✓ Silence gaps are visible on timeline                 │
└─────────────────────────────────────────────────────────┘
```

---

## 4. Tokenized Input

After tokenization:

```
┌───────────────────────────────────────────────────────────┐
│  Tokenizer Output                                         │
│                                                           │
│  input_ids: [101, 2054, 2003, ..., 102, ...]            │
│             └─────────────────────────┘                   │
│             ~342 tokens total                             │
│                                                           │
│  attention_mask: [1, 1, 1, ..., 1, ...]                 │
│  offset_mapping: [(0, 0), (0, 3), (4, 7), ...]          │
│                                                           │
│  Token breakdown:                                         │
│  ├─ Token 0:     101 = [CLS]                              │
│  ├─ Token 1-3:   "You" "are" "a" (system prompt)         │
│  ├─ ...                                                   │
│  ├─ Token K:     103 = [SEP] (turn separator)             │
│  ├─ ...                                                   │
│  └─ Token N:     102 = [EOS]                              │
│                                                           │
│  SANITY CHECK:                                            │
│  ✓ input_ids length ≈ 300-400 tokens (reasonable)        │
│  ✓ Special tokens present ([CLS], [SEP], [EOS])          │
│  ✓ offset_mapping aligns with ranges from step 3         │
│  ✓ No padding added yet (handled in batch collation)     │
└───────────────────────────────────────────────────────────┘
```

---

## 5. Label Tensors from Range Mapping

After converting character ranges to token-level labels:

```
┌───────────────────────────────────────────────────────────┐
│  Label Tensors (same shape as input_ids)                 │
│                                                           │
│  speaking_labels: [0, 0, 0, ..., 1, 1, ..., 0, ...]    │
│                                                           │
│  Breakdown:                                               │
│  ├─ Position 0-K:   Value 0 (system + other text)        │
│  ├─ Position K-M:   Value 1 (assistant turn text)        │
│  ├─ Position M-N:   Value 0 (other turns)                │
│  └─ Position N+:    Value 0 (end of sequence)            │
│                                                           │
│  speaking_gen_labels: [-100, -100, ..., tok_id, ...]    │
│                                                           │
│  Breakdown:                                               │
│  ├─ Non-assistant positions: -100 (ignored in loss)      │
│  ├─ Assistant positions:     token_id (next token pred)  │
│  └─ -100 means "don't train on this position"            │
│                                                           │
│  dst_update_labels: similar structure for DST_UPDATE     │
│  dst_gen_labels:    similar structure for DST_UPDATE     │
│                                                           │
│  SANITY CHECK:                                            │
│  ✓ Labels shape = input_ids shape                         │
│  ✓ 1s correspond to correct turn types (assistant/DST)   │
│  ✓ -100 used for positions to ignore in training         │
│  ✓ Each turn's text is labeled consistently              │
└───────────────────────────────────────────────────────────┘
```

---

## 6. Frame Embedding Alignment

After extracting frame embeddings in conversation order:

```
┌───────────────────────────────────────────────────────────┐
│  Flattened Frame Embeddings by Turn Order                │
│                                                           │
│  Turn 1 (system, frames 0-2):                             │
│  ├─ Embedding[0]: [2048-dim]  ← Frame 0                  │
│  ├─ Embedding[1]: [2048-dim]  ← Frame 1                  │
│  └─ Embedding[2]: [2048-dim]  ← Frame 2                  │
│                                                           │
│  Turn 2 (DST_UPDATE, frames 186-190):                    │
│  ├─ Embedding[3]: [2048-dim]  ← Frame 186                │
│  ├─ Embedding[4]: [2048-dim]  ← Frame 187                │
│  ├─ Embedding[5]: [2048-dim]  ← Frame 188                │
│  ├─ Embedding[6]: [2048-dim]  ← Frame 189                │
│  └─ Embedding[7]: [2048-dim]  ← Frame 190                │
│                                                           │
│  Turn 3 (user, frames 186-190):                          │
│  ├─ Embedding[8]: [2048-dim]  ← Frame 186                │
│  ... (same as Turn 2 since frames overlap)               │
│                                                           │
│  Final shape: [total_frames_used, 2048]                  │
│  Example: [29, 2048] if 29 unique frames referenced      │
│                                                           │
│  SANITY CHECK:                                            │
│  ✓ All embeddings exist (no frame index out of bounds)   │
│  ✓ Embedding order matches frame order in text           │
│  ✓ Shape is [frames_used, 2048] (not [total_frames])    │
│  ✓ No missing embeddings for any referenced frame        │
└───────────────────────────────────────────────────────────┘
```

---

## 7. Final Batch Ready for Model

After batch collation:

```
┌───────────────────────────────────────────────────────────┐
│  Batch Dictionary Sent to Model                          │
│                                                           │
│  batch = {                                                │
│    "input_ids":              [batch_size, seq_len],      │
│    "attention_mask":         [batch_size, seq_len],      │
│    "image_embeds":           [total_frames, 2048],       │
│    "speaking_labels":        [batch_size, seq_len],      │
│    "speaking_gen_labels":    [batch_size, seq_len],      │
│    "dst_update_labels":      [batch_size, seq_len],      │
│    "dst_gen_labels":         [batch_size, seq_len],      │
│  }                                                       │
│                                                           │
│  Example with batch_size=4, seq_len=512:                 │
│  ├─ input_ids:        [4, 512]                            │
│  ├─ attention_mask:   [4, 512]                            │
│  ├─ image_embeds:     [116, 2048]  (total frames in batch)│
│  ├─ speaking_labels:  [4, 512]                            │
│  └─ ...labels:        [4, 512]                            │
│                                                           │
│  SANITY CHECK:                                            │
│  ✓ Batch size correct                                     │
│  ✓ Sequence length consistent across all tensors         │
│  ✓ Label shapes match input_ids shape                    │
│  ✓ image_embeds dimension is 2048 (correct embedding dim)│
│  ✓ No NaN values in any tensor                            │
│  ✓ Value ranges are reasonable (embeddings normalized)   │
└───────────────────────────────────────────────────────────┘
```

---

## 8. Visualization Generation Plan

This section describes the code that will be written to **generate all diagrams from actual data**.

### 8.1 Input Diagram Generator

**Purpose:** Generate Section 3 (Timeline visualization) from actual sample data

**Component:** `InputDiagramGenerator` class

**Inputs:**
- Sample dict from dataset: `{conversation, embeddings, ...}`
- Frame range (e.g., [0, 258])
- Frame display density (every N frames on timeline)

**Outputs:**
- ASCII timeline showing frame numbers and turn boundaries
- Frame numbers are actual frame indices from the data
- Color coding or role markers (S/D/U/A) for each turn
- Silence gaps clearly marked

**Example Output:**
```
Frame:    0   1   2  ... 185 186 187 188 189 190 191 192 193 194 ... 258
          |   |   |       |   |   |   |   |   |   |   |   |   |       |
Turn 0:   [S] [S] [S]
          "You are a helpful..."

SILENCE:           (frames 3-185 not in any turn)

Turn 1:                        [D] [D] [D] [D] [D]
                               "[DST_UPDATE] S1: start"

Turn 2:                        [U] [U] [U] [U] [U]
                               "I want to assemble..."

Turn 3:                            [A] [A] [A] [A] [A]
                                   "Great! Let's..."

SILENCE:                                           (frames 195-258 not in any turn)
```

### 8.2 Silence Modeling Visualization

**Purpose:** Show how silence/gaps are handled in the model input

**Questions to answer:**
- Which frames between turns are **not included** in any turn?
- Are silence frames:
  - Completely skipped?
  - Marked with special SILENCE tokens?
  - Included as negative samples?
  - Used for background modeling?

**Output:** A chart showing:
```
SILENCE ANALYSIS:
├─ Total frames in video: 259
├─ Frames used by turns: 87
├─ Silence frames: 172
├─ Silence gaps:
│  ├─ Gap 1 (frames 3-185):   183 frames - SILENCE
│  ├─ Gap 2 (frames 195-258):  64 frames - SILENCE
│  └─ Overlapping regions:     Some turns overlap (shared frames)
└─ How silence is modeled:
   ├─ Option A: Sparse (completely skip silence frames)
   ├─ Option B: Negative sampling (train NOT to speak)
   └─ Option C: SILENCE token (add special marker)
```

### 8.3 Token-Frame Alignment Visualization

**Purpose:** Show how tokens map back to frames

**Output:** A table showing token ranges and their corresponding frame indices:
```
Turn          | Token Range | Frame Range   | Content
──────────────┼─────────────┼───────────────┼──────────────────
system        | 0-14        | [0,1,2]       | You are helpful...
DST_UPDATE    | 15-20       | [186-190]     | S1: start
user          | 21-38       | [186-190]     | I want to assemble...
assistant     | 39-60       | [187-191]     | Great! Let's...
```

### 8.4 Formatted Text Visualization

**Purpose:** Show actual formatted text with frame numbers (not generic `<image>`)

**Output:** Show the concatenated text as it appears before tokenization:
```
FORMATTED TEXT (with frame numbers):

<frame_0><frame_1><frame_2>You are a helpful assistant. Always be ready to assist.

<frame_186><frame_187><frame_188><frame_189><frame_190>[DST_UPDATE] S1: start

<frame_186><frame_187><frame_188><frame_189><frame_190>I want to assemble a toy vehicle with a turntable and cabin.

<frame_187><frame_188><frame_189><frame_190><frame_191>Great! Let's get started. First, attach the interior to the chassis.

...
```

### 8.5 Implementation Components

| Class/Function | Responsibility |
|---|---|
| `InputDiagramGenerator.generate_timeline()` | Create Section 3 (timeline with frame numbers) |
| `InputDiagramGenerator.generate_silence_analysis()` | Create silence gap visualization |
| `InputDiagramGenerator.generate_token_frame_map()` | Create token-to-frame alignment table |
| `InputDiagramGenerator.generate_formatted_text()` | Show actual formatted text with frame numbers |
| `InputDiagramGenerator.visualize_sample()` | Orchestrate all visualizations for one sample |

### 8.6 Usage

```python
from custom.src.prospect.utils.input_diagram_generator import InputDiagramGenerator

# Load sample
dataset = DSTTrainingDataset(...)
sample = dataset[0]

# Generate visualizations
generator = InputDiagramGenerator(sample)
diagrams = {
    "timeline": generator.generate_timeline(),
    "silence": generator.generate_silence_analysis(),
    "token_frame_map": generator.generate_token_frame_map(),
    "formatted_text": generator.generate_formatted_text(),
}

# Print or save
for name, diagram in diagrams.items():
    print(f"\n=== {name.upper()} ===")
    print(diagram)
```

### 8.7 Output Formats

- **Console/Terminal:** ASCII art and tables (direct print)
- **Markdown:** Formatted code blocks with frame info
- **HTML:** Interactive timeline with tooltips (future enhancement)
- **PDF:** Exportable report (future enhancement)

---

## 9. Quick Sanity Checks

Use these checks to verify data correctness:

### Dataset Level
- [ ] Total samples loaded: X
- [ ] Samples with empty conversation: 0
- [ ] Samples with frame range errors: 0
- [ ] Embeddings all shape [N, 2048]: ✓
- [ ] No NaN in embeddings: ✓
- [ ] Frame ranges are within bounds: ✓

### Temporal Alignment Level
- [ ] Frame numbers increase monotonically within turns: ✓
- [ ] No gaps within a turn (unless sparse): ✓
- [ ] Turns are ordered by start_frame: ✓
- [ ] Overlapping turns have same frame indices: ✓

### Silence Modeling Level
- [ ] Gaps between turns are identified correctly: ✓
- [ ] Silence frames handling is consistent: ✓
- [ ] No frames are double-counted: ✓

### Tokenization Level
- [ ] Token count reasonable (150-500 per sample): ✓
- [ ] Special tokens present: ✓
- [ ] Offset mappings align with character ranges: ✓
- [ ] No -100 tokens in input_ids (only in labels): ✓

### Label Level
- [ ] Label shapes = input_ids shapes: ✓
- [ ] Label values in {-100, 0, 1}: ✓
- [ ] Speaking labels have at least one 1: ✓
- [ ] DST labels have at least one 1: ✓
- [ ] -100 tokens are ignored in loss: ✓

### Batch Level
- [ ] All samples in batch have same padded length: ✓
- [ ] Image embeds from all turns present: ✓
- [ ] No misalignment between tokens and frames: ✓
- [ ] Attention masks are all 1s in valid region: ✓

---

## 10. Script Locations and Next Steps

### To Create:
```
custom/src/prospect/utils/input_diagram_generator.py
  └─ InputDiagramGenerator class with visualization methods

scripts/visualize_model_inputs.py
  └─ CLI script for generating diagrams from actual samples

notebooks/visualize_model_inputs.ipynb
  └─ Jupyter notebook for interactive exploration
```

### To Run:

```bash
# Generate visualizations for sample 0
python scripts/visualize_model_inputs.py \
    --data_path custom/outputs/dst_generated/hybrid_dst/2025-11-30/00-56-27_gpt-4o_proassist_10rows/assembly101/train_training.json \
    --sample_idx 0 \
    --output_dir custom/outputs/visualizations/

# This will create:
# ├─ sample_0_timeline.txt
# ├─ sample_0_silence_analysis.txt
# ├─ sample_0_token_frame_map.txt
# ├─ sample_0_formatted_text.txt
# └─ sample_0_all.txt (combined)
```

### Implementation Priority:
1. **Phase 1 (Essential):** Timeline with frame numbers, silence analysis
2. **Phase 2 (Important):** Token-frame mapping, formatted text visualization
3. **Phase 3 (Nice-to-have):** HTML interactive visualizations, batch-level analysis
