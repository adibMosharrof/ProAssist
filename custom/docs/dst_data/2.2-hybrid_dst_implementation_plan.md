# ðŸ“˜ Hybrid DST Label Generation Implementation Plan

This document outlines the complete implementation plan for replacing the existing flawed DST generation approach with the new hybrid two-stage algorithm defined in `2.1-proassist_dst_label_plan_changes.md`.

---

## ðŸŽ¯ Implementation Objectives

2. **Implement New Hybrid Algorithm**: Build the two-stage pipeline with global similarity scoring and LLM fallback
3. **Clean Codebase**: Remove unused imports, classes, and dependencies
4. **Maintain Compatibility**: Ensure the pipeline continues to work with existing infrastructure

---

## ðŸ“‹ Phase 1: Replace DST Generator (Minimal Changes)

### 1.1 Simple Replacement Strategy

**NEW APPROACH**: Instead of removing old code, simply replace the generator class:
- **Current**: `ProAssistDSTLabelGenerator` (flawed semantic/NLI approach)
- **Replace with**: `HybridDSTLabelGenerator` (new hybrid approach)
- **Benefits**: Minimal code changes, same interface, drop-in replacement

### 1.2 Keep Dependencies (Reuse for New Approach)

**IMPORTANT**: Keep these dependencies as they're still used in the new hybrid approach:
- `sentence-transformers` (used for semantic similarity in Phase 1)
- `cross-encoder` (used for NLI scoring in Phase 1)
- Any other ML libraries used for the old approach (will be reused)

**What's Different**:
- Old approach: Used these for ALL cases with monotonic DP
- New approach: Uses these for high-confidence cases only (~70%), LLM fallback for ambiguous cases (~30%)

### 1.3 Update SimpleDSTGenerator (Minimal Changes)

**In `simple_dst_generator.py` (lines 39-46)**:
**BEFORE**:
```python
# Initialize DST generator
self.dst_generator = ProAssistDSTLabelGenerator(
    model_name=cfg.model.name,
    temperature=cfg.model.temperature,
    max_tokens=cfg.model.max_tokens,
    max_retries=cfg.max_retries,
)
```

**AFTER**:
```python
# Initialize Hybrid DST generator
self.dst_generator = HybridDSTLabelGenerator(
    model_name=cfg.model.name,
    temperature=cfg.model.temperature,
    max_tokens=cfg.model.max_tokens,
    max_retries=cfg.max_retries,
)
```

### 1.4 Update Import

**In `simple_dst_generator.py` (line 11)**:
**BEFORE**:
```python
from dst_data_builder.gpt_generators.proassist_label_generator import (
    ProAssistDSTLabelGenerator,
)
```

**AFTER**:
```python
from dst_data_builder.gpt_generators.hybrid_dst_generator import (
    HybridDSTLabelGenerator,
)
```

### 1.5 No Other Changes Required

**The following remain unchanged**:
- `DSTDataProcessor` initialization (line 52)
- All training modules integration
- Training data creation pipeline
- All existing configuration and logging

---

## ðŸ“‹ Phase 2: New Implementation Architecture

### 2.1 File Structure

**New Hybrid DST Implementation** will be created in:
```
custom/src/dst_data_builder/hybrid_dst/
â”œâ”€â”€ __init__.py                    # Module initialization
â”œâ”€â”€ hybrid_dst_generator.py        # Main HybridDSTLabelGenerator class
â”œâ”€â”€ overlap_aware_reducer.py       # OverlapAwareBlockReducer class
â”œâ”€â”€ hybrid_span_constructor.py     # HybridSpanConstructor class
â”œâ”€â”€ global_similarity_calculator.py # GlobalSimilarityCalculator class
â”œâ”€â”€ llm_ambiguous_handler.py       # LLMAmbiguousHandler class
â””â”€â”€ temporal_validator.py          # TemporalOrderingValidator class
```

**Reuse from existing training modules:**
- `dst_data_builder/training_modules/dst_enums.py` - DSTTransition, DSTState, DSTRole, DSTLabel enums
- `dst_data_builder/training_modules/conversation_splitter.py` - DST state tracking and validation logic
- `dst_data_builder/training_modules/system_prompts.py` - Existing prompt templates

### 2.3 Test Structure

**Tests will follow the pattern from `training_module` tests:**

```
custom/src/dst_data_builder/tests/hybrid_dst/
â”œâ”€â”€ __init__.py                              # Test module initialization
â”œâ”€â”€ test_hybrid_dst_modules.py              # Individual module tests
â””â”€â”€ test_hybrid_dst_integration.py          # End-to-end integration tests
```

**Test pattern from `training_module/test_training_modules.py`:**
- **Test class structure**: `TestHybridDSTModules` class with pytest fixtures
- **Sample data creation**: `create_sample_config()` and `create_sample_video_data()`
- **Individual module tests**: `test_*_only()` methods for each component
- **Integration tests**: `test_module_integration()` for full pipeline testing
- **Comprehensive assertions**: Both positive and negative test cases
- **Verbose logging**: Print statements with emoji for clear test output

### 2.2 Core Implementation Classes

#### **HybridDSTLabelGenerator** (New Main Class)
```python
class HybridDSTLabelGenerator:
    """Two-stage hybrid DST label generation using global similarity and LLM fallback"""

    def __init__(self, config: DictConfig):
        self.config = config
        self.overlap_reducer = OverlapAwareBlockReducer(config.hybrid_dst)
        self.span_constructor = HybridSpanConstructor(config.hybrid_dst, config.model)
        self.validator = TemporalOrderingValidator(config.hybrid_dst)
        self.logger = logging.getLogger(__name__)

    def generate_dst_labels(self, all_step_descriptions: List[Dict], inferred_knowledge: List[str]) -> List[Dict]:
        total_steps = len(all_step_descriptions)
        self.logger.info(f"ðŸš€ Starting hybrid DST label generation for {total_steps} step descriptions")

        # Overlap-Aware Block Reduction: Block reduction
        filtered_blocks = self.overlap_reducer.reduce_blocks(all_step_descriptions)
        filtered_count = len(filtered_blocks)
        self.logger.info(f"ðŸ“¦ Block reduction: {total_steps} â†’ {filtered_count} filtered blocks")

        # Hybrid DST Span Construction: Two-phase boundary detection
        dst_spans = self.span_constructor.construct_spans(filtered_blocks, inferred_knowledge)

        # Validation
        validated_spans = self.validator.validate_temporal_ordering(dst_spans)
        validated_count = len(validated_spans)
        self.logger.info(f"âœ… Temporal validation: {len(dst_spans)} â†’ {validated_count} validated spans")

        self.logger.info(f"ðŸŽ‰ Hybrid DST generation complete: {validated_count} final DST labels")
        return validated_spans
```

#### **OverlapAwareBlockReducer** (Overlap-Aware Block Reduction)
```python
class OverlapAwareBlockReducer:
    """Overlap-Aware Block Reduction: Merge contained blocks using time overlap only"""
    
    def reduce_blocks(self, all_step_descriptions: List[Dict]) -> List[Dict]:
        # Identify main blocks with time ranges
        # Find blocks contained within main blocks
        # Merge contained blocks into main blocks
        # Return filtered_blocks
```

#### **HybridSpanConstructor** (Hybrid DST Span Construction)
```python
class HybridSpanConstructor:
    """Hybrid DST Span Construction: Two-phase boundary detection"""

    def __init__(self, hybrid_config: DictConfig, model_config: DictConfig):
        self.similarity_calculator = GlobalSimilarityCalculator(hybrid_config.two_phase_algorithm)
        self.llm_handler = LLMAmbiguousHandler(model_config)
        self.logger = logging.getLogger(__name__)

    def construct_spans(self, filtered_blocks: List[Dict], inferred_knowledge: List[str]) -> List[Dict]:
        total_blocks = len(filtered_blocks)
        self.logger.info(f"ðŸ” Processing {total_blocks} filtered blocks for DST span construction")

        # High-Confidence Global Similarity Scoring: Matrix-based scoring
        clear_blocks, ambiguous_blocks = self.similarity_calculator.score_blocks(filtered_blocks, inferred_knowledge)

        clear_count = len(clear_blocks)
        ambiguous_count = len(ambiguous_blocks)

        self.logger.info(f"ðŸ“Š Matrix scoring: {clear_count}/{total_blocks} clear blocks ({clear_count/total_blocks:.1%}), "
                        f"{ambiguous_count}/{total_blocks} ambiguous blocks ({ambiguous_count/total_blocks:.1%})")

        # LLM Fallback for Ambiguous Cases: Advanced reasoning for unclear boundaries
        llm_decisions = []
        if ambiguous_blocks:
            self.logger.info(f"ðŸ¤– Triggering LLM fallback for {ambiguous_count} ambiguous blocks")
            llm_decisions = self.llm_handler.resolve_ambiguous_blocks(ambiguous_blocks, inferred_knowledge)
        else:
            self.logger.info("âœ… No ambiguous blocks found - using matrix scoring only")

        # Combine decisions and construct final spans
        final_spans = self.combine_decisions(clear_blocks, ambiguous_blocks, llm_decisions)
        self.logger.info(f"ðŸŽ¯ Constructed {len(final_spans)} final DST spans")

        return final_spans
```

#### **GlobalSimilarityCalculator** (High-Confidence Global Similarity Scoring)
```python
class GlobalSimilarityCalculator:
    """High-Confidence Global Similarity Scoring: Matrix-based global similarity scoring"""

    def __init__(self, config: DictConfig):
        self.semantic_weight = config.semantic_similarity_weight
        self.nli_weight = config.nli_score_weight
        self.high_confidence_threshold = config.high_confidence_threshold

    def score_blocks(self, filtered_blocks: List[Dict], inferred_knowledge: List[str]) -> Tuple[List, List]:
        # Compute semantic similarity matrix
        semantic_matrix = self.compute_semantic_similarity_matrix(filtered_blocks, inferred_knowledge)

        # Compute NLI score matrix
        nli_matrix = self.compute_nli_score_matrix(filtered_blocks, inferred_knowledge)

        # Combine matrices
        combined_matrix = self.combine_scores(semantic_matrix, nli_matrix)

        # Find best matches and confidence gaps
        return self.separate_clear_from_ambiguous(combined_matrix)
```

#### **LLMAmbiguousHandler** (LLM Fallback for Ambiguous Cases)
```python
class LLMAmbiguousHandler:
    """LLM Fallback for Ambiguous Cases: LLM fallback for ambiguous cases with comprehensive logging"""

    def __init__(self, config: DictConfig):
        # Use existing model configuration from Hydra config
        self.llm_client = OpenAIClient(config.model)
        self.logger = logging.getLogger(__name__)

    def resolve_ambiguous_blocks(self, ambiguous_blocks: List[Dict], inferred_knowledge: List[str]) -> List[str]:
        num_ambiguous = len(ambiguous_blocks)
        self.logger.info(f"ðŸ¤– LLM Fallback: Processing {num_ambiguous} ambiguous blocks")

        # Prepare context for all ambiguous blocks
        contexts = self.prepare_contexts(ambiguous_blocks, inferred_knowledge)

        # Log LLM call details
        self.logger.info(f"ðŸ“¡ Making {len(contexts)} LLM calls for ambiguous case resolution")
        for i, context in enumerate(contexts[:3]):  # Log first 3 contexts
            self.logger.debug(f"LLM Context {i+1}: {context[:200]}...")

        # Batch query LLM
        responses = self.llm_client.batch_query(contexts)

        self.logger.info(f"âœ… Received {len(responses)} LLM responses")

        # Parse responses and extract decisions
        decisions = self.parse_decisions(responses)

        self.logger.info(f"ðŸŽ¯ LLM resolved {len(decisions)} ambiguous blocks into {len(set(decisions))} unique decisions")
        return decisions
```

#### **TemporalOrderingValidator** (Validation)
```python
class TemporalOrderingValidator:
    """Validate that DST spans respect temporal ordering"""
    
    def validate_temporal_ordering(self, dst_spans: List[Dict]) -> List[Dict]:
        # Check timestamp increasing: kb[i].t1 <= kb[i+1].t0
        # Handle violations (regression vs overlap)
        # Return validated spans
```

### 2.2 Integration Points

#### **Update DSTDataProcessor**
```python
class DSTDataProcessor:
    def __init__(self, hybrid_dst_generator: HybridDSTLabelGenerator, **kwargs):
        self.hybrid_dst_generator = hybrid_dst_generator
        # Remove old dst_generator parameter
    
    def process_dataset_split(self, dataset_name: str, split: str, num_rows: int, output_dir: Path):
        # Use hybrid_dst_generator instead of dst_generator
        dst_labels = self.hybrid_dst_generator.generate_dst_labels(all_step_descriptions, inferred_knowledge)
```

#### **Update SimpleDSTGenerator**
```python
class SimpleDSTGenerator:
    def __init__(self, cfg: DictConfig):
        # Remove old DST generator initialization
        # self.dst_generator = ProAssistDSTLabelGenerator(...)  # DELETE THIS

        # Add new hybrid DST generator
        self.hybrid_dst_generator = HybridDSTLabelGenerator(cfg)

        # Update data processor initialization
        self.data_processor = DSTDataProcessor(
            hybrid_dst_generator=self.hybrid_dst_generator,
            # Remove dst_generator parameter
        )
```

---

## ðŸ“‹ Phase 3: Configuration Updates

### 3.1 Optional New Configuration

**If hybrid-specific settings are needed, add to `simple_dst_generator.yaml`**:
```yaml
# OPTIONAL: Hybrid DST Label Generation
hybrid_dst:
  # Hybrid DST Span Construction
  two_phase_algorithm:
    high_confidence_threshold: 0.3
    semantic_similarity_weight: 0.6
    nli_score_weight: 0.4

  # Temporal Validation
  validation:
    max_allowed_overlap: 5.0  # seconds

  # Matrix Operations
  matrix_computation:
    batch_size: 32
    use_gpu: true
```

**Note**: The new generator should work with existing model configuration from Hydra defaults (e.g., `model: gpt4o` with temperature and max_tokens defined in `model/gpt4o.yaml`). No removal of old configuration is required.

---

## ðŸ“‹ Phase 4: Testing and Validation

### 4.1 Test Implementation Structure

**Create tests following training module pattern:**

```python
# custom/src/dst_data_builder/tests/hybrid_dst/test_hybrid_dst_modules.py
class TestHybridDSTModules:
    """Test class for hybrid DST modules integration"""
    
    def test_overlap_aware_block_reduction():
        # Test Overlap-Aware Block Reduction: block merging logic
        
    def test_global_similarity_scoring():
        # Test High-Confidence Global Similarity Scoring: matrix operations and confidence calculation
        
    def test_llm_ambiguous_handler():
        # Test LLM Fallback for Ambiguous Cases: LLM query and response parsing
        
    def test_temporal_ordering_validation():
        # Test validation logic and violation handling
        
    def test_module_integration():
        # Test complete pipeline with sample data
```

### 4.2 Integration Tests

```python
# custom/src/dst_data_builder/tests/hybrid_dst/test_hybrid_dst_integration.py
def test_end_to_end_dst_generation():
    # Test complete pipeline with sample data
    
```

**Test pattern features:**
- **Pytest fixtures**: `config` and `video_data` fixtures
- **Sample data creation**: `create_sample_config()` and `create_sample_video_data()`
- **Verbose logging**: Print statements with emoji for clear output
- **Comprehensive assertions**: Both positive and negative test cases
- **Individual module tests**: `test_*_only()` methods for each component
- **Integration testing**: End-to-end pipeline validation

---

## ðŸ“‹ Phase 5: Migration Steps

### Step 1: Create Hybrid DST Implementation Folder
1. Read `.kilocode/rules/coding_convention.md` to understand coding conventions
2. Create `custom/src/dst_data_builder/hybrid_dst/` directory
3. Implement all required classes in separate files for modularity

### Step 2: Implement Core Classes
1. Create `hybrid_dst_generator.py` - Main `HybridDSTLabelGenerator` class
2. Create `overlap_aware_reducer.py` - `OverlapAwareBlockReducer` class
3. Create `hybrid_span_constructor.py` - `HybridSpanConstructor` class
4. Create `global_similarity_calculator.py` - `GlobalSimilarityCalculator` class
5. Create `llm_ambiguous_handler.py` - `LLMAmbiguousHandler` class
6. Create `temporal_validator.py` - `TemporalOrderingValidator` class

### Step 3: Reuse Existing Training Modules
1. Import and utilize `dst_enums.py` for DST transitions and states
2. Leverage `conversation_splitter.py` for state tracking logic
3. Use `system_prompts.py` for existing prompt templates

### Step 4: Simple Class Replacement
1. Update import in `simple_dst_generator.py` (line 11)
2. Update class name in `simple_dst_generator.py` (line 40)
3. Test that the interface works the same way

### Step 5: Test and Validate
1. Run existing tests to ensure compatibility
2. Test with sample data using the new generator
3. Validate output format matches expectations


## ðŸ“‹ Phase 6: Code Removal Checklist

### Files to Delete:
- [ ] `custom/src/dst_data_builder/gpt_generators/proassist_label_generator.py`
- [ ] Any test files specific to the old approach

### Imports to Remove:
- [ ] `from dst_data_builder.gpt_generators.proassist_label_generator import ProAssistDSTLabelGenerator`
- [ ] Sentence transformers imports
- [ ] Cross-encoder imports
- [ ] Any other old ML library imports

### Classes/Methods to Remove:
- [ ] `ProAssistDSTLabelGenerator` class
- [ ] Semantic similarity computation methods
- [ ] NLI scoring methods  
- [ ] Monotonic DP alignment methods
- [ ] Old parsing and timestamp inference logic

### Configuration to Remove:
- [ ] Semantic similarity model configuration
- [ ] NLI model configuration
- [ ] Monotonic DP parameters
- [ ] Old threshold settings

### Dependencies to Keep (Reuse for New Approach):
- [x] `sentence-transformers` (still used in Phase 1 for semantic similarity)
- [x] `cross-encoder` (still used in Phase 1 for NLI scoring)
- [x] Any other ML libraries used for the old approach (reused in new hybrid approach)

**Note**: The difference is in HOW these libraries are used:
- **Old approach**: Used for ALL cases with monotonic DP alignment
- **New approach**: Used for high-confidence cases only (~70%), LLM fallback for ambiguous cases (~30%)

---

## ðŸŽ¯ Success Criteria

### Functionality
- [ ] New hybrid algorithm produces correct DST labels
- [ ] Temporal ordering validation works correctly
- [ ] LLM fallback handles ambiguous cases
- [ ] Output format compatible with existing pipeline

### Performance
- [ ] Matrix operations are efficient (GPU acceleration when available)
- [ ] LLM usage minimized (~30% of cases vs 100% in old approach)
- [ ] Overall processing time improved or maintained
- [ ] Comprehensive logging for LLM usage monitoring and debugging

### Code Quality
- [ ] Clean separation of concerns between stages
- [ ] Comprehensive error handling and logging
- [ ] Detailed LLM usage logging for monitoring and cost tracking
- [ ] Well-documented code with clear interfaces
- [ ] Full test coverage for new components

### Compatibility
- [ ] Existing training pipeline continues to work
- [ ] Configuration system updated properly
- [ ] No breaking changes to downstream components

---

## ðŸ“‹ Phase 6: Code Removal Checklist

### Files to Delete:
- [ ] `custom/src/dst_data_builder/gpt_generators/proassist_label_generator.py`
- [ ] Any test files specific to the old approach

### Imports to Remove:
- [ ] `from dst_data_builder.gpt_generators.proassist_label_generator import ProAssistDSTLabelGenerator`
- [ ] Sentence transformers imports
- [ ] Cross-encoder imports
- [ ] Any other old ML library imports

### Classes/Methods to Remove:
- [ ] `ProAssistDSTLabelGenerator` class
- [ ] Semantic similarity computation methods
- [ ] NLI scoring methods
- [ ] Monotonic DP alignment methods
- [ ] Old parsing and timestamp inference logic

### Configuration to Remove:
- [ ] Semantic similarity model configuration
- [ ] NLI model configuration
- [ ] Monotonic DP parameters
- [ ] Old threshold settings

### Dependencies to Keep (Reuse for New Approach):
- [x] `sentence-transformers` (still used in Phase 1 for semantic similarity)
- [x] `cross-encoder` (still used in Phase 1 for NLI scoring)
- [x] Any other ML libraries used for the old approach (reused in new hybrid approach)

**Note**: The difference is in HOW these libraries are used:
- **Old approach**: Used for ALL cases with monotonic DP alignment
- **New approach**: Used for high-confidence cases only (~70%), LLM fallback for ambiguous cases (~30%)

---

## ðŸ”„ Rollback Plan

If issues arise during implementation:

1. **Immediate Rollback**: Change class name back to `ProAssistDSTLabelGenerator`
2. **Partial Rollback**: Keep new `HybridDSTLabelGenerator` class but don't replace old one
3. **Interface Rollback**: Ensure new class has same methods as old one for drop-in compatibility

---

**Implementation Timeline**: 1-2 days
**Risk Level**: Low (simple class replacement with same interface)
**Testing Required**: Basic compatibility testing and functionality validation