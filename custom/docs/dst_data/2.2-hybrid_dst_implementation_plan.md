# ðŸ“˜ Hybrid DST Label Generation Implementation Plan

This document outlines the complete implementation plan for replacing the existing flawed DST generation approach with the new hybrid two-stage algorithm defined in `2.1-proassist_dst_label_plan_changes.md`.

---

## ðŸŽ¯ Implementation Objectives

1. **Remove Existing Flawed Implementation**: Delete all code related to semantic similarity, NLI scoring, and monotonic DP
2. **Implement New Hybrid Algorithm**: Build the two-stage pipeline with global similarity scoring and LLM fallback
3. **Clean Codebase**: Remove unused imports, classes, and dependencies
4. **Maintain Compatibility**: Ensure the pipeline continues to work with existing infrastructure

---

## ðŸ“‹ Phase 1: Code Cleanup and Removal

### 1.1 Delete Old Implementation Files

**Remove these files completely**:
- `custom/src/dst_data_builder/gpt_generators/proassist_label_generator.py` (contains flawed semantic/NLI approach)

### 1.2 Keep Dependencies (Reuse for New Approach)

**IMPORTANT**: Keep these dependencies as they're still used in the new hybrid approach:
- `sentence-transformers` (used for semantic similarity in Phase 1)
- `cross-encoder` (used for NLI scoring in Phase 1)
- Any other ML libraries used for the old approach (will be reused)

**What's Different**:
- Old approach: Used these for ALL cases with monotonic DP
- New approach: Uses these for high-confidence cases only (~70%), LLM fallback for ambiguous cases (~30%)

### 1.3 Clean Up Imports and Usage

**In `simple_dst_generator.py`**:
- Remove import: `from dst_data_builder.gpt_generators.proassist_label_generator import ProAssistDSTLabelGenerator`
- Remove DST generator initialization (lines 39-46)
- Update `DSTDataProcessor` initialization to not require `dst_generator`

### 1.4 Remove Unused Training Modules

**Identify and remove training modules that were specific to the old approach**:
- Any modules that processed semantic similarity scores
- NLI-related processing components
- Monotonic DP alignment components

---

## ðŸ“‹ Phase 2: New Implementation Architecture

### 2.1 Core Implementation Classes

#### **HybridDSTLabelGenerator** (New Main Class)
```python
class HybridDSTLabelGenerator:
    """Two-stage hybrid DST label generation using global similarity and LLM fallback"""

    def __init__(self, config: DictConfig):
        self.config = config
        self.overlap_reducer = OverlapAwareBlockReducer(config.hybrid_dst)
        self.span_constructor = HybridSpanConstructor(config.hybrid_dst, config.model)
        self.validator = TemporalOrderingValidator(config.hybrid_dst)
        self.logger = logging.getLogger(__name__)

    def generate_dst_labels(self, all_step_descriptions: List[Dict], inferred_knowledge: List[str]) -> List[Dict]:
        total_steps = len(all_step_descriptions)
        self.logger.info(f"ðŸš€ Starting hybrid DST label generation for {total_steps} step descriptions")

        # Overlap-Aware Block Reduction: Block reduction
        filtered_blocks = self.overlap_reducer.reduce_blocks(all_step_descriptions)
        filtered_count = len(filtered_blocks)
        self.logger.info(f"ðŸ“¦ Block reduction: {total_steps} â†’ {filtered_count} filtered blocks")

        # Hybrid DST Span Construction: Two-phase boundary detection
        dst_spans = self.span_constructor.construct_spans(filtered_blocks, inferred_knowledge)

        # Validation
        validated_spans = self.validator.validate_temporal_ordering(dst_spans)
        validated_count = len(validated_spans)
        self.logger.info(f"âœ… Temporal validation: {len(dst_spans)} â†’ {validated_count} validated spans")

        self.logger.info(f"ðŸŽ‰ Hybrid DST generation complete: {validated_count} final DST labels")
        return validated_spans
```

#### **OverlapAwareBlockReducer** (Overlap-Aware Block Reduction)
```python
class OverlapAwareBlockReducer:
    """Overlap-Aware Block Reduction: Merge contained blocks using time overlap only"""
    
    def reduce_blocks(self, all_step_descriptions: List[Dict]) -> List[Dict]:
        # Identify main blocks with time ranges
        # Find blocks contained within main blocks
        # Merge contained blocks into main blocks
        # Return filtered_blocks
```

#### **HybridSpanConstructor** (Hybrid DST Span Construction)
```python
class HybridSpanConstructor:
    """Hybrid DST Span Construction: Two-phase boundary detection"""

    def __init__(self, hybrid_config: DictConfig, model_config: DictConfig):
        self.similarity_calculator = GlobalSimilarityCalculator(hybrid_config.two_phase_algorithm)
        self.llm_handler = LLMAmbiguousHandler(model_config)
        self.logger = logging.getLogger(__name__)

    def construct_spans(self, filtered_blocks: List[Dict], inferred_knowledge: List[str]) -> List[Dict]:
        total_blocks = len(filtered_blocks)
        self.logger.info(f"ðŸ” Processing {total_blocks} filtered blocks for DST span construction")

        # High-Confidence Global Similarity Scoring: Matrix-based scoring
        clear_blocks, ambiguous_blocks = self.similarity_calculator.score_blocks(filtered_blocks, inferred_knowledge)

        clear_count = len(clear_blocks)
        ambiguous_count = len(ambiguous_blocks)

        self.logger.info(f"ðŸ“Š Matrix scoring: {clear_count}/{total_blocks} clear blocks ({clear_count/total_blocks:.1%}), "
                        f"{ambiguous_count}/{total_blocks} ambiguous blocks ({ambiguous_count/total_blocks:.1%})")

        # LLM Fallback for Ambiguous Cases: Advanced reasoning for unclear boundaries
        llm_decisions = []
        if ambiguous_blocks:
            self.logger.info(f"ðŸ¤– Triggering LLM fallback for {ambiguous_count} ambiguous blocks")
            llm_decisions = self.llm_handler.resolve_ambiguous_blocks(ambiguous_blocks, inferred_knowledge)
        else:
            self.logger.info("âœ… No ambiguous blocks found - using matrix scoring only")

        # Combine decisions and construct final spans
        final_spans = self.combine_decisions(clear_blocks, ambiguous_blocks, llm_decisions)
        self.logger.info(f"ðŸŽ¯ Constructed {len(final_spans)} final DST spans")

        return final_spans
```

#### **GlobalSimilarityCalculator** (High-Confidence Global Similarity Scoring)
```python
class GlobalSimilarityCalculator:
    """High-Confidence Global Similarity Scoring: Matrix-based global similarity scoring"""

    def __init__(self, config: DictConfig):
        self.semantic_weight = config.semantic_similarity_weight
        self.nli_weight = config.nli_score_weight
        self.high_confidence_threshold = config.high_confidence_threshold

    def score_blocks(self, filtered_blocks: List[Dict], inferred_knowledge: List[str]) -> Tuple[List, List]:
        # Compute semantic similarity matrix
        semantic_matrix = self.compute_semantic_similarity_matrix(filtered_blocks, inferred_knowledge)

        # Compute NLI score matrix
        nli_matrix = self.compute_nli_score_matrix(filtered_blocks, inferred_knowledge)

        # Combine matrices
        combined_matrix = self.combine_scores(semantic_matrix, nli_matrix)

        # Find best matches and confidence gaps
        return self.separate_clear_from_ambiguous(combined_matrix)
```

#### **LLMAmbiguousHandler** (LLM Fallback for Ambiguous Cases)
```python
class LLMAmbiguousHandler:
    """LLM Fallback for Ambiguous Cases: LLM fallback for ambiguous cases with comprehensive logging"""

    def __init__(self, config: DictConfig):
        # Use existing model configuration from Hydra config
        self.llm_client = OpenAIClient(config.model)
        self.logger = logging.getLogger(__name__)

    def resolve_ambiguous_blocks(self, ambiguous_blocks: List[Dict], inferred_knowledge: List[str]) -> List[str]:
        num_ambiguous = len(ambiguous_blocks)
        self.logger.info(f"ðŸ¤– LLM Fallback: Processing {num_ambiguous} ambiguous blocks")

        # Prepare context for all ambiguous blocks
        contexts = self.prepare_contexts(ambiguous_blocks, inferred_knowledge)

        # Log LLM call details
        self.logger.info(f"ðŸ“¡ Making {len(contexts)} LLM calls for ambiguous case resolution")
        for i, context in enumerate(contexts[:3]):  # Log first 3 contexts
            self.logger.debug(f"LLM Context {i+1}: {context[:200]}...")

        # Batch query LLM
        responses = self.llm_client.batch_query(contexts)

        self.logger.info(f"âœ… Received {len(responses)} LLM responses")

        # Parse responses and extract decisions
        decisions = self.parse_decisions(responses)

        self.logger.info(f"ðŸŽ¯ LLM resolved {len(decisions)} ambiguous blocks into {len(set(decisions))} unique decisions")
        return decisions
```

#### **TemporalOrderingValidator** (Validation)
```python
class TemporalOrderingValidator:
    """Validate that DST spans respect temporal ordering"""
    
    def validate_temporal_ordering(self, dst_spans: List[Dict]) -> List[Dict]:
        # Check timestamp increasing: kb[i].t1 <= kb[i+1].t0
        # Handle violations (regression vs overlap)
        # Return validated spans
```

### 2.2 Integration Points

#### **Update DSTDataProcessor**
```python
class DSTDataProcessor:
    def __init__(self, hybrid_dst_generator: HybridDSTLabelGenerator, **kwargs):
        self.hybrid_dst_generator = hybrid_dst_generator
        # Remove old dst_generator parameter
    
    def process_dataset_split(self, dataset_name: str, split: str, num_rows: int, output_dir: Path):
        # Use hybrid_dst_generator instead of dst_generator
        dst_labels = self.hybrid_dst_generator.generate_dst_labels(all_step_descriptions, inferred_knowledge)
```

#### **Update SimpleDSTGenerator**
```python
class SimpleDSTGenerator:
    def __init__(self, cfg: DictConfig):
        # Remove old DST generator initialization
        # self.dst_generator = ProAssistDSTLabelGenerator(...)  # DELETE THIS

        # Add new hybrid DST generator
        self.hybrid_dst_generator = HybridDSTLabelGenerator(cfg)

        # Update data processor initialization
        self.data_processor = DSTDataProcessor(
            hybrid_dst_generator=self.hybrid_dst_generator,
            # Remove dst_generator parameter
        )
```

---

## ðŸ“‹ Phase 3: Configuration Updates

### 3.1 New Configuration Structure

**Add to `simple_dst_generator.yaml`**:
```yaml
# NEW: Hybrid DST Label Generation
hybrid_dst:
  # Hybrid DST Span Construction
  two_phase_algorithm:
    high_confidence_threshold: 0.3
    semantic_similarity_weight: 0.6
    nli_score_weight: 0.4

  # Temporal Validation
  validation:
    max_allowed_overlap: 5.0  # seconds

  # Matrix Operations
  matrix_computation:
    batch_size: 32
    use_gpu: true
```

**Note**: Uses existing model configuration from Hydra defaults (e.g., `model: gpt4o` with temperature and max_tokens defined in `model/gpt4o.yaml`)

### 3.2 Remove Old Configuration

**Remove from existing config**:
- Any semantic similarity model configuration
- NLI model configuration  
- Monotonic DP parameters
- Old threshold settings

---

## ðŸ“‹ Phase 4: Testing and Validation

### 4.1 Unit Tests

**Create tests for new components**:
```python
# tests/test_hybrid_dst_generator.py
def test_overlap_aware_block_reduction():
    # Test Overlap-Aware Block Reduction: block merging logic
    
def test_global_similarity_scoring():
    # Test High-Confidence Global Similarity Scoring: matrix operations and confidence calculation
    
def test_llm_ambiguous_handler():
    # Test LLM Fallback for Ambiguous Cases: LLM query and response parsing
    
def test_temporal_ordering_validation():
    # Test validation logic and violation handling
```

### 4.2 Integration Tests

```python
# tests/test_hybrid_dst_integration.py  
def test_end_to_end_dst_generation():
    # Test complete pipeline with sample data
```

### 4.3 Performance Tests

```python
# Performance comparison with old approach
def test_performance_vs_old_approach():
    # Verify new approach is faster and more accurate
```

---

## ðŸ“‹ Phase 5: Migration Steps

### Step 1: Backup Current Implementation
```bash
# Create backup before making changes
cp -r custom/src/dst_data_builder/gpt_generators/ backup_old_dst_generators/
```

### Step 2: Remove Old Code
1. Delete `proassist_label_generator.py`
2. Remove unused imports from `simple_dst_generator.py`
3. Update `DSTDataProcessor` initialization

### Step 3: Implement New Components
1. Create `HybridDSTLabelGenerator` class
2. Implement Overlap-Aware Block Reduction: `OverlapAwareBlockReducer`
3. Implement Hybrid DST Span Construction: `HybridSpanConstructor` with both phases
4. Add `TemporalOrderingValidator`

### Step 4: Update Configuration
1. Add new hybrid DST configuration to YAML files
2. Remove old configuration parameters
3. Update configuration validation

### Step 5: Test and Validate
1. Run unit tests for new components
2. Run integration tests
3. Test with sample data
4. Validate output format compatibility


## ðŸ“‹ Phase 6: Code Removal Checklist

### Files to Delete:
- [ ] `custom/src/dst_data_builder/gpt_generators/proassist_label_generator.py`
- [ ] Any test files specific to the old approach

### Imports to Remove:
- [ ] `from dst_data_builder.gpt_generators.proassist_label_generator import ProAssistDSTLabelGenerator`
- [ ] Sentence transformers imports
- [ ] Cross-encoder imports
- [ ] Any other old ML library imports

### Classes/Methods to Remove:
- [ ] `ProAssistDSTLabelGenerator` class
- [ ] Semantic similarity computation methods
- [ ] NLI scoring methods  
- [ ] Monotonic DP alignment methods
- [ ] Old parsing and timestamp inference logic

### Configuration to Remove:
- [ ] Semantic similarity model configuration
- [ ] NLI model configuration
- [ ] Monotonic DP parameters
- [ ] Old threshold settings

### Dependencies to Keep (Reuse for New Approach):
- [x] `sentence-transformers` (still used in Phase 1 for semantic similarity)
- [x] `cross-encoder` (still used in Phase 1 for NLI scoring)
- [x] Any other ML libraries used for the old approach (reused in new hybrid approach)

**Note**: The difference is in HOW these libraries are used:
- **Old approach**: Used for ALL cases with monotonic DP alignment
- **New approach**: Used for high-confidence cases only (~70%), LLM fallback for ambiguous cases (~30%)

---

## ðŸŽ¯ Success Criteria

### Functionality
- [ ] New hybrid algorithm produces correct DST labels
- [ ] Temporal ordering validation works correctly
- [ ] LLM fallback handles ambiguous cases
- [ ] Output format compatible with existing pipeline

### Performance
- [ ] Matrix operations are efficient (GPU acceleration when available)
- [ ] LLM usage minimized (~30% of cases vs 100% in old approach)
- [ ] Overall processing time improved or maintained
- [ ] Comprehensive logging for LLM usage monitoring and debugging

### Code Quality
- [ ] Clean separation of concerns between stages
- [ ] Comprehensive error handling and logging
- [ ] Detailed LLM usage logging for monitoring and cost tracking
- [ ] Well-documented code with clear interfaces
- [ ] Full test coverage for new components

### Compatibility
- [ ] Existing training pipeline continues to work
- [ ] Configuration system updated properly
- [ ] No breaking changes to downstream components

---

## ðŸ”„ Rollback Plan

If issues arise during implementation:

1. **Immediate Rollback**: Restore from backup created in Step 1
2. **Partial Rollback**: Keep new Stage 1, rollback Stage 2 to use simpler logic
3. **Configuration Rollback**: Revert configuration changes while keeping new classes

---

**Implementation Timeline**: 2-3 days
**Risk Level**: Medium (due to removal of existing implementation)
**Testing Required**: Extensive unit and integration testing before deployment