# ðŸ§  Training Plan: Vision-Language Progress Summarization with DST Grounding

---

## 1. Overview

### ðŸŽ¯ Goal  
Develop a **Vision-Language Model (VLM)** that can:

1. Observe **video frames** and **dialogue context** in an egocentric instructional task.  
2. Interpret **task structure** via a predefined **Dialog State Tree (DST)**.  
3. Predict which steps/substeps/actions are **Completed (C)**, **In Progress (IP)**, or **Not Started (NS)**.  
4. Generate a **structured progress summary** describing what has been accomplished, what is ongoing, and what remains.  
5. Optionally, **point to visual evidence** (frames) that justify its predictions.

---

## 2. Motivation

Traditional **ProAssist** used:
- A **large LLaMA-based LLM** for language reasoning.  
- A separate **image encoder** for visual perception.  
- A **4K-token summarization window** for context.  

Our approach:
- Replace with a **single lightweight VLM** (e.g., `SmolVLM2-2.2B-Instruct`).  
- Use **joint vision-language embeddings** for alignment.  
- Achieve comparable or better results with **1â€“2K tokens** by incorporating **DST structure** and **temporal grounding**.

---

## 3. Core Learning Objectives

The model jointly learns **three complementary tasks**:

| Objective | Description | Output |
|------------|--------------|---------|
| **A. DST State Prediction** | Predict C/IP/NS for each step, substep, or action node. | Per-node state labels |
| **B. Evidence Grounding (Pointer Head)** | Identify which frames support each active node. | Frame indices or timestamps |
| **C. Progress Summarization** | Generate a structured JSON summary + concise progress note. | JSON text |

These objectives make the model both **accurate** and **interpretable**.

---

## 4. Data Representation

### 4.1 Inputs

Each training sample (a video window) contains:

| Component | Example | Tokens/Frames |
|------------|----------|---------------|
| **Video frames** | 16â€“24 frames @ 1 fps (e.g., 97sâ€“112s) | Vision input |
| **DST schema** | List of steps/substeps/actions with text descriptions | 300â€“500 tokens |
| **Dialogue context** | Last 10â€“15 turns (summarized) | â‰¤500 tokens |
| **Memory JSON** | Previous summary of the task | â‰¤200 tokens |
| **Captions / Hints** | Optional object-verb captions | â‰¤100 tokens |

---

### 4.2 Labels

Derived automatically from your annotated DST JSON:

- **Node states**: `y_state[i] âˆˆ {C, IP, NS}` at time Ï„  
  (using the rule: `C if end_ts â‰¤ Ï„; IP if start_ts â‰¤ Ï„ < end_ts; else NS`)
- **Evidence frames**: indices `y_frame[i]` = frames within `[start_ts, end_ts]`
- **Summary JSON**: ground-truth structured summary for the window

---

## 5. Model Architecture

A **multi-head VLM** built on a pretrained base like `SmolVLM2-2.2B-Instruct`.

### 5.1 Base VLM
- Vision encoder (ViT-G/14) â†’ patch embeddings.  
- Text decoder with cross-modal attention.  
- Already trained for image/videoâ€“text alignment.

### 5.2 Added Components

#### 1ï¸âƒ£ DST Node Encoder
Encodes each DST node:
```
"S2.1: Attach wheel to chassis"
```
â†’ hidden vector `v_i`.

Used as conditioning tokens in both classification and generation.

#### 2ï¸âƒ£ Graph State Head
- Input: pooled visual + node embeddings  
- Output: per-node 3-way logits (C/IP/NS)  
- Loss: Cross-Entropy per node

#### 3ï¸âƒ£ Evidence Pointer Head
- Input: node embeddings `v_i` and frame embeddings `f_j`  
- Output: attention distribution `p_{i,j} = softmax_j(sim(v_i, f_j))`  
- Loss: CE alignment with ground-truth frames

#### 4ï¸âƒ£ Progress Generator
Language decoder that generates:
- Structured JSON (steps completed, in-progress, blocked)
- Natural-language progress note (â‰¤120 tokens)

---

## 6. Training Objectives

Total loss:
```
L = Î»1 * L_state + Î»2 * L_evidence + Î»3 * L_summary
```

### A. State Loss
```
L_state = Î£_i CE(y_state[i], Å·_state[i])
```

### B. Evidence Loss
```
L_evidence = -Î£_i Î£_j y_frame[i,j] * log(p[i,j])
```

### C. Summary Loss
```
L_summary = CE(y_json_tokens, Å·_json_tokens)
```

**Recommended weights:**  
`Î»1 = 1.0`, `Î»2 = 0.5`, `Î»3 = 1.0`

---

## 7. Training Setup

| Setting | Value |
|----------|--------|
| **Model** | SmolVLM2-2.2B-Instruct |
| **Precision** | bfloat16 |
| **Batch size** | 2â€“4 windows per GPU |
| **LoRA ranks** | 16â€“32 |
| **Trainable layers** | Top 8 language blocks + cross-modal layers |
| **Learning rate** | 1e-4 with cosine decay |
| **Optimizer** | AdamW |
| **Epochs** | 1â€“2 (early stop on val F1 of current step) |

---

## 8. Forward Pass Example

**Input (time window 116.5sâ€“152.1s):**
- 20 frames (wheel assembly phase)
- DST: S1â€“S6 text descriptions
- Previous memory JSON
- Optional captions: â€œhand holding wheelâ€, â€œscrewdriver tightening wheelâ€

**Model sees:**
```
[Frames]
[DST Nodes: S1...S6]
[Dialogue Context]
[Memory JSON]
â†’ Encoded via SmolVLM2 backbone
```

**Outputs:**
```text
S1: Completed
S2: In Progress
S3â€“S6: Not Started
```

**Evidence pointer:**
- For node S2.1 (â€œAttach wheelâ€), points to frames near 123.7sâ€“146.8s.

**Progress summary:**
```json
{
  "completed_steps": ["S1_Assemble chassis"],
  "current_step": {
    "step_id": "S2_Attach wheels",
    "evidence": [{"frame": 5, "t": "124s"}, {"frame": 10, "t": "139s"}]
  },
  "next_actions": ["Attach arm connector to arm"],
  "progress_note": "Chassis assembly completed. Now attaching wheels to the chassis using a screwdriver."
}
```

---

## 9. Evaluation Metrics

| Category | Metric | Description |
|-----------|---------|-------------|
| **DST State Prediction** | Accuracy, F1 | Node-level C/IP/NS classification |
| **Current Step Accuracy** | Accuracy | Most active step at Ï„ |
| **Evidence Grounding** | Frame F1 (Â±2s) | Overlap between predicted & gold frames |
| **Progress Summarization** | BLEU / ROUGE / Human Utility | Faithfulness, usefulness |
| **Efficiency** | Tokens vs. performance | Compare 1K / 2K / 4K context |
| **Interpretability** | Qualitative | Nodeâ†’Frame heatmaps |

---

## 10. Evaluation Example

**Gold:**
- Active: S2.1 (Attach wheel)  
- Evidence: frames 123sâ€“147s  
- Summary: â€œAttaching wheels to chassis with screwdriver.â€

**Model Output:**
- Active: S2.1  
- Evidence: frames 124s, 139s  
- Summary: â€œWheels are being attached to the chassis using a screwdriver.â€

âœ… Step correctness: **True**  
âœ… Frame overlap: **2/2 correct (F1 = 1.0)**  
âœ… Summary: **Semantically faithful**

---

## 11. Why This Works

- âœ… **DST as structure** â†’ model updates states, not rebuilds trees.  
- âœ… **Evidence pointer** â†’ explicit visual grounding.  
- âœ… **JSON outputs** â†’ structured, factual summaries under tight context.  
- âœ… **LoRA fine-tuning** â†’ efficient on 2Ã—V100 GPUs.  
- âœ… **Evaluation-ready** â†’ measurable interpretability.

---

## 12. Optional Extensions

| Idea | Description |
|------|--------------|
| **Curriculum training** | Train state â†’ add summarization later. |
| **Contrastive alignment** | Add InfoNCE between node/frame embeddings. |
| **Pseudo-labeling** | Use larger VLM (Qwen2.5-VL-7B) as teacher. |
| **Memory updates** | Feed previous JSON summaries as state. |
| **Heatmap visualizer** | Display node-frame attention for interpretability. |

---

## 13. Training Flow Summary

**Step 1 â€” Data Preparation:**  
- Slide windows across video (16â€“24 frames).  
- Derive DST node labels and frame evidence.  
- Generate progress summaries.

**Step 2 â€” Model Input:**  
- Encode video, DST schema, dialogue, and memory.

**Step 3 â€” Multitask Learning:**  
- Predict node states.  
- Identify evidence frames.  
- Generate structured summary.

**Step 4 â€” Joint Optimization:**  
- Combine `L_state + L_evidence + L_summary` with tuned Î»â€™s.

**Step 5 â€” Evaluation:**  
- Quantitative: F1, frame grounding, summarization metrics.  
- Qualitative: visual grounding, examples, ablations.

---

## 14. Expected Outcomes

A **lightweight, explainable** progress summarizer that:

- Operates within **1.5â€“2K tokens**.  
- Accurately tracks **DST-based task progression**.  
- Produces **concise structured summaries**.  
- Grounds claims in **visual evidence**.  
- Scales efficiently on **SmolVLM2-2.2B** or similar models.

---

**End of Document**
