# DST ProAssist Training Configuration

model:
  # llm_pretrained: "meta-llama/Llama-3.2-3B-Instruct"
  llm_pretrained: "meta-llama/Llama-3.2-1B-Instruct"
  vision_hidden_size: 1152  # SigLIP output dimension
  max_seq_len: 4096
  binary_decision_head_type: "linear"
  binary_loss_weight: 1.0
  # Generation head architecture (binary decision heads always enabled)
  use_separate_generation_heads: false  # If true: use lm_head (speaking) + dst_generation_head (DST); if false: use single lm_head
  # Quantization settings
  use_int4_quantization: false  # Must be false for full fine-tuning (use_lora: false). Set to true only with LoRA.
  bnb_4bit_compute_dtype: "bfloat16"  # Compute dtype for quantized model
  bnb_4bit_quant_type: "nf4"  # Quantization type: "nf4" or "fp4"
  bnb_4bit_use_double_quant: true  # Use double quantization for extra memory savings

lora:
  # use_lora: true
  use_lora: false
  lora_r: 128
  lora_alpha: 256
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"


data:
  data_dir: custom/outputs/dst_generated/sparse_format/2025-12-06/14-13-11_gpt-4o_proassist_sparse
  dataset_name: "assembly101"
  siglip_features_dir: null  # Will default to data_dir if not specified
  negative_sampling_rate: 0.1  # Sample 50% of negative frames to balance classes

training:
  output_dir: "checkpoints"
  num_epochs: 2
  batch_size: 4
  eval_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 2e-5
  warmup_steps: 100
  logging_steps: 10
  save_steps: 1
  eval_steps: 1
  save_total_limit: 2
  bf16: true
  fp16: false
  num_workers: 0
  report_to: wandb
  dataloader_drop_last: true
  gradient_checkpointing: true
  ddp_find_unused_parameters: false

# Hydra configuration
hydra:
  run:
    dir: custom/outputs/dst_proassist_training/${now:%Y-%m-%d}/${now:%H-%M}
  job:
    chdir: false
